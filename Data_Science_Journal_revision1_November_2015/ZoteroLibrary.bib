

@book{biggs2011teaching,
  title={Teaching for quality learning at university},
  author={Biggs, John and Tang, Catherine},
  year={2011},
  publisher={McGraw-Hill Education (UK)}
}

@article{biggs2003aligning,
  title={Aligning teaching and assessment to curriculum objectives},
  author={Biggs, John},
  journal={Imaginative Curriculum Project, LTSN Generic Centre},
  volume={12},
  date={2003}
}

@article{Hastings1970,
  title = {Monte Carlo sampling methods using Markov chains and their applications},
  volume = {57},
  issn = {0006-3444, 1464-3510},
  url = {http://biomet.oxfordjournals.org/content/57/1/97},
  doi = {10.1093/biomet/57.1.97},
  abstract = {SUMMARY A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.},
  timestamp = {2014-12-19 21:36:40},
  number = {1},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  author = {Hastings, W. K.},
  urldate = {2014-11-25},
  date = {1970-04-01},
  pages = {97--109},
  langid = {english},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/QUSRTTHK/Hastings - 1970 - Monte Carlo sampling methods using Markov chains a.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/GMA8C59F/97.html:text/html}
}

@article{Hoffman2014,
  title = {The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo},
  volume = {15},
  timestamp = {2015-03-25 23:18:05},
  journaltitle = {Journal of Machine Learning Research},
  shortjournal = {J. Mach. Learn. Res.},
  author = {Hoffman, Matthew D and Gelman, Andrew},
  date = {2014},
  pages = {1593--1623}
}

@article{Gimenez2007,
  title = {State-space modelling of data on marked individuals},
  volume = {206},
  issn = {0304-3800},
  url = {http://www.sciencedirect.com/science/article/pii/S0304380007001925},
  doi = {10.1016/j.ecolmodel.2007.03.040},
  abstract = {State-space models have recently been proposed as a convenient and flexible framework for specifying stochastic models for the dynamics of wild animal populations. Here we focus on the modelling of data on marked individuals which is frequently used in order to estimate demographic parameters while accounting for imperfect detectability. We show how usual models to deal with capture–recapture and ring-recovery data can be fruitfully written as state-space models. An illustration is given using real data and a Bayesian approach using MCMC methods is implemented to estimate the parameters. Eventually, we discuss future developments that may be facilitated by the SSM formulation.},
  timestamp = {2014-12-19 21:36:54},
  number = {3–4},
  journaltitle = {Ecological Modelling},
  shortjournal = {Ecological Modelling},
  author = {Gimenez, Olivier and Rossi, Vivien and Choquet, Rémi and Dehais, Camille and Doris, Blaise and Varella, Hubert and Vila, Jean-Pierre and Pradel, Roger},
  urldate = {2014-09-24},
  date = {2007-08-24},
  pages = {431--438},
  keywords = {Bayesian inference,Capture–recapture,Cormack–Jolly–Seber,MCMC methods,Multistate models,Ring-recovery models,Survival estimation,WinBUGS},
  file = {ScienceDirect Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/3CXP2A37/Gimenez et al. - 2007 - State-space modelling of data on marked individual.pdf:application/pdf;ScienceDirect Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/AM59MU2A/Gimenez et al. - 2007 - State-space modelling of data on marked individual.html:text/html}
}

@book{burns_cambridge_2012,
  title = {The Cambridge Guide to Pedagogy and Practice in Second Language Teaching},
  isbn = {978-1-107-01586-9},
  abstract = {This collection of original articles provides a state-of-the-art overview of key issues and approaches in contemporary language teaching. Written by internationally prominent researchers, educators, and emerging scholars, the chapters are grouped into five sections: rethinking our understanding of teaching, learner diversity and classroom learning, pedagogical approaches and practices, components of the curriculum, and media and materials. Each chapter covers key topics in teaching methodology such as reflective pedagogy, teaching large classes, outcomes-based language learning, speaking instruction, and technology in the classroom. Chapters assume no particular background knowledge and are written in an accessible style.},
  pagetotal = {313},
  timestamp = {2015-07-10 18:52:07},
  publisher = {Cambridge University Press},
  author = {Burns, Anne and Richards, Jack C.},
  date = {2012-01-31},
  langid = {english},
  keywords = {Education / Bilingual Education,Foreign Language Study / English as a Second Language,Language Arts \& Disciplines / General}
}

@article{Horn2013,
  title = {The transformational potential of flipped classrooms},
  volume = {13},
  timestamp = {2015-07-07 18:55:43},
  number = {3},
  journaltitle = {Education Next},
  shortjournal = {Educ. Next},
  author = {Horn, Michael},
  date = {2013},
  pages = {78--79},
  year = 2013
}

@book{Schlotzhauer1997,
  title = {{SAS} system for elementary statistical analysis},
  timestamp = {2015-03-05 07:13:41},
  publisher = {{SAS} institute Cary, North Carolina},
  author = {Schlotzhauer, Sandra D and Littell, Ramon C},
  date = {1997}
}

@article{Caffo2005,
  title = {Ascent-Based Monte Carlo Expectation-Maximization},
  volume = {67},
  issn = {13697412},
  timestamp = {2015-03-04 22:30:17},
  number = {2},
  journaltitle = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  shortjournal = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  author = {Caffo, Brian S. and Jank, Wolfgang and Jones, Galin L.},
  date = {2005},
  pages = {235--251},
  note = {ArticleType: primary\_article / Full publication date: 2005 / Copyright © 2005 Royal Statistical Society},
  file = {Caffo, Jank, and Jones, 2005.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/S9KQR3AM/Caffo, Jank, and Jones, 2005.pdf:application/pdf}
}

@collection{RCoreTeam2014,
  location = {Vienna, Austria},
  title = {R: A Language and Environment for Statistical Computing},
  timestamp = {2014-12-19 21:36:50},
  publisher = {R Foundation for Statistical Computing},
  editor = {{R Core Team}},
  date = {2014},
  year = 2014
}

@article{Neal2011,
  title = {{MCMC} using Hamiltonian dynamics},
  volume = {2},
  timestamp = {2014-12-19 21:36:37},
  journaltitle = {Handbook of Markov Chain Monte Carlo},
  shortjournal = {Handb. Markov Chain Monte Carlo},
  author = {Neal, Radford},
  date = {2011}
}

@article{Cai2011,
  title = {Adaptive Thresholding for Sparse Covariance Matrix Estimation},
  volume = {106},
  issn = {0162-1459},
  abstract = {In this article we consider estimation of sparse covariance matrices and propose a thresholding procedure that is adaptive to the variability of individual entries. The estimators are fully data-driven and demonstrate excellent performance both theoretically and numerically. It is shown that the estimators adaptively achieve the optimal rate of convergence over a large class of sparse covariance matrices under the spectral norm. In contrast, the commonly used universal thresholding estimators are shown to be suboptimal over the same parameter spaces. Support recovery is discussed as well. The adaptive thresholding estimators are easy to implement. The numerical performance of the estimators is studied using both simulated and real data. Simulation results demonstrate that the adaptive thresholding estimators uniformly outperform the universal thresholding estimators. The method is also illustrated in an analysis on a dataset from a small round blue-cell tumor microarray experiment. A supplement to this article presenting additional technical proofs is available online.},
  timestamp = {2015-03-04 22:30:29},
  number = {494},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {J. Am. Stat. Assoc.},
  author = {Cai, Tony and Liu, Weidong},
  date = {2011-06-01},
  pages = {672--684},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/GHR72A84/Cai and Liu - 2011 - Adaptive Thresholding for Sparse Covariance Matrix.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/ZWZZ45T6/jasa.2011.html:text/html}
}

@article{Stern2000,
  title = {Posterior predictive model checks for disease mapping models},
  volume = {19},
  rights = {Copyright © 2000 John Wiley \& Sons, Ltd.},
  issn = {1097-0258},
  url = {http://onlinelibrary.wiley.com/doi/10.1002/1097-0258(20000915/30)19:17/18<2377::AID-SIM576>3.0.CO;2-1/abstract},
  doi = {10.1002/1097-0258(20000915/30)19:17/18<2377::AID-SIM576>3.0.CO;2-1},
  abstract = {Disease incidence or disease mortality rates for small areas are often displayed on maps. Maps of raw rates, disease counts divided by the total population at risk, have been criticized as unreliable due to non-constant variance associated with heterogeneity in base population size. This has led to the use of model-based Bayes or empirical Bayes point estimates for map creation. Because the maps have important epidemiological and political consequences, for example, they are often used to identify small areas with unusually high or low unexplained risk, it is important that the assumptions of the underlying models be scrutinized. We review the use of posterior predictive model checks, which compare features of the observed data to the same features of replicate data generated under the model, for assessing model fitness. One crucial issue is whether extrema are potentially important epidemiological findings or merely evidence of poor model fit. We propose the use of the cross-validation posterior predictive distribution, obtained by reanalysing the data without a suspect small area, as a method for assessing whether the observed count in the area is consistent with the model. Because it may not be feasible to actually reanalyse the data for each suspect small area in large data sets, two methods for approximating the cross-validation posterior predictive distribution are described. Copyright © 2000 John Wiley \& Sons, Ltd.},
  timestamp = {2014-12-19 21:36:47},
  number = {17-18},
  journaltitle = {Statistics in Medicine},
  shortjournal = {Statist. Med.},
  author = {Stern, Hal S. and Cressie, Noel},
  urldate = {2014-12-16},
  date = {2000},
  pages = {2377--2397},
  langid = {english},
  file = {Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/CQW6TGVV/abstract.html:text/html;Stern and Cressie, 2000.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/Q6WJTRKJ/Stern and Cressie, 2000.pdf:application/pdf}
}

@article{Roberts2009,
  title = {Examples of Adaptive {MCMC}},
  volume = {18},
  issn = {1061-8600},
  url = {http://dx.doi.org/10.1198/jcgs.2009.06134},
  doi = {10.1198/jcgs.2009.06134},
  abstract = {We investigate the use of adaptive MCMC algorithms to automatically tune the Markov chain parameters during a run. Examples include the Adaptive Metropolis (AM) multivariate algorithm of Haario, Saksman, and Tamminen (2001), Metropolis-within-Gibbs algorithms for nonconjugate hierarchical models, regionally adjusted Metropolis algorithms, and logarithmic scalings. Computer simulations indicate that the algorithms perform very well compared to nonadaptive algorithms, even in high dimension.},
  timestamp = {2014-12-19 21:36:23},
  number = {2},
  journaltitle = {Journal of Computational and Graphical Statistics},
  shortjournal = {J. Comput. Graph. Stat.},
  author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
  urldate = {2014-12-01},
  date = {2009-01-01},
  pages = {349--367},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/BQ4694TF/Roberts and Rosenthal - 2009 - Examples of Adaptive MCMC.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/N8JBS3CN/jcgs.2009.html:text/html}
}

@article{Winston1986,
  title = {{LISP}. Second edition},
  timestamp = {2015-03-04 22:25:03},
  author = {Winston, P. H. and Horn, B. K.},
  date = {1986-01-01},
  langid = {english},
  file = {Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/WM87TMGI/7203980.html:text/html}
}

@inproceedings{Rompf2010,
  location = {New York, {NY}, {USA}},
  title = {Lightweight Modular Staging: A Pragmatic Approach to Runtime Code Generation and Compiled {DSLs}},
  isbn = {978-1-4503-0154-1},
  doi = {10.1145/1868294.1868314},
  shorttitle = {Lightweight Modular Staging},
  abstract = {Software engineering demands generality and abstraction, performance demands specialization and concretization. Generative programming can provide both, but the effort required to develop high-quality program generators likely offsets their benefits, even if a multi-stage programming language is used. We present lightweight modular staging, a library-based multi-stage programming approach that breaks with the tradition of syntactic quasi-quotation and instead uses only types to distinguish between binding times. Through extensive use of component technology, lightweight modular staging makes an optimizing compiler framework available at the library level, allowing programmers to tightly integrate domain-specific abstractions and optimizations into the generation process. We argue that lightweight modular staging enables a form of language virtualization, i.e. allows to go from a pure-library embedded language to one that is practically equivalent to a stand-alone implementation with only modest effort.},
  timestamp = {2015-03-04 22:24:40},
  booktitle = {Proceedings of the Ninth International Conference on Generative Programming and Component Engineering},
  series = {{GPCE} '10},
  publisher = {{ACM}},
  author = {Rompf, Tiark and Odersky, Martin},
  date = {2010},
  pages = {127--136},
  keywords = {code generation,domain-specific languages,language virtualization,multi-stage programming},
  file = {ACM Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/VTURBRIX/Rompf and Odersky - 2010 - Lightweight Modular Staging A Pragmatic Approach .pdf:application/pdf}
}

@book{Spiegelhalter2003,
  title = {{WinBUGS} user manual},
  url = {http://www.politicalbubbles.org/bayes_beach/manual14.pdf},
  timestamp = {2014-12-19 21:37:06},
  publisher = {version},
  author = {Spiegelhalter, David and Thomas, Andrew and Best, Nicky and Lunn, Dave},
  urldate = {2014-12-08},
  date = {2003},
  file = {[PDF] from politicalbubbles.org:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/N699TNXJ/Spiegelhalter et al. - 2003 - WinBUGS user manual.pdf:application/pdf}
}

@article{Skaug2006,
  title = {Automatic approximation of the marginal likelihood in non-Gaussian hierarchical models},
  volume = {51},
  issn = {0167-9473},
  url = {http://www.sciencedirect.com/science/article/pii/S0167947306000764},
  doi = {10.1016/j.csda.2006.03.005},
  abstract = {Fitting of non-Gaussian hierarchical random effects models by approximate maximum likelihood can be made automatic to the same extent that Bayesian model fitting can be automated by the program BUGS. The word “automatic” means that the technical details of computation are made transparent to the user. This is achieved by combining a technique from computer science known as “automatic differentiation” with the Laplace approximation for calculating the marginal likelihood. Automatic differentiation, which should not be confused with symbolic differentiation, is mostly unknown to statisticians, and hence basic ideas and results are reviewed. The computational performance of the approach is compared to that of existing mixed-model software on a suite of datasets selected from the mixed-model literature.},
  timestamp = {2015-01-09 00:02:33},
  number = {2},
  journaltitle = {Computational Statistics \& Data Analysis},
  shortjournal = {Computational Statistics \& Data Analysis},
  author = {Skaug, Hans J. and Fournier, David A.},
  urldate = {2015-01-09},
  date = {2006-11-15},
  pages = {699--709},
  keywords = {AD Model Builder,Automatic differentiation,Importance sampling,Laplace approximation,Mixed models,Random effects},
  file = {ScienceDirect Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/8VU8APKN/Skaug and Fournier - 2006 - Automatic approximation of the marginal likelihood.pdf:application/pdf;ScienceDirect Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/PCCVF9W5/S0167947306000764.html:text/html}
}

@article{Gelfand2005,
  title = {Bayesian Nonparametric Spatial Modeling With Dirichlet Process Mixing},
  volume = {100},
  issn = {0162-1459},
  url = {http://amstat.tandfonline.com/doi/abs/10.1198/016214504000002078},
  doi = {10.1198/016214504000002078},
  timestamp = {2015-03-08 03:29:38},
  number = {471},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  author = {Gelfand, Alan E and Kottas, Athanasios and {MacEachern}, Steven N},
  urldate = {2015-03-08},
  date = {2005-09-01},
  pages = {1021--1035},
  file = {3-7-2015_Bayesian N.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/85VEUS6N/3-7-2015_Bayesian N.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/D9RB7WJM/login.html:text/html}
}

@article{Andrieu2010,
  title = {Particle markov chain monte carlo methods},
  volume = {72},
  timestamp = {2015-07-17 06:31:06},
  number = {3},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  shortjournal = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  author = {Andrieu, Christophe and Doucet, Arnaud and Holenstein, Roman},
  date = {2010},
  pages = {269--342}
}

@incollection{liu_combined_2001,
  title = {Combined parameter and state estimation in simulation-based filtering},
  timestamp = {2015-07-21 18:33:06},
  booktitle = {Sequential Monte Carlo methods in practice},
  publisher = {Springer},
  author = {Liu, Jane and West, Mike},
  date = {2001},
  pages = {197--223}
}

@article{Eddelbuettel2011,
  title = {Rcpp: Seamless R and C++ integration},
  volume = {40},
  timestamp = {2015-03-02 19:54:40},
  number = {8},
  journaltitle = {Journal of Statistical Software},
  shortjournal = {J. Stat. Softw.},
  author = {Eddelbuettel, Dirk and Fraņcois, Romain and Allaire, J and Chambers, John and Bates, Douglas and Ushey, Kevin},
  date = {2011},
  pages = {1--18}
}

@inproceedings{kamat2012agile,
  title = {Agile Practices in Higher Education: A Case Study},
  timestamp = {2015-08-05 22:23:05},
  booktitle = {{AGILE} India ({AGILE} {INDIA}), 2012},
  publisher = {{IEEE}},
  author = {Kamat, Venkatesh and Sardessai, Shailaja},
  date = {2012},
  pages = {48--55}
}

@book{Lunn2012,
  title = {The {BUGS} Book: A Practical Introduction to Bayesian Analysis},
  shorttitle = {The {BUGS} Book},
  abstract = {Bayesian statistical methods have become widely used for data analysis and modelling in recent years, and the BUGS software has become the most popular software for Bayesian analysis worldwide. Authored by the team that originally developed this software, The BUGS Book provides a practical introduction to this program and its use. The text presents complete coverage of all the functionalities of BUGS, including prediction, missing data, model criticism, and prior sensitivity. It also features a large number of worked examples and a wide range of applications from various disciplines.  The book introduces regression models, techniques for criticism and comparison, and a wide range of modelling issues before going into the vital area of hierarchical models, one of the most common applications of Bayesian methods. It deals with essentials of modelling without getting bogged down in complexity. The book emphasises model criticism, model comparison, sensitivity analysis to alternative priors, and thoughtful choice of prior distributions—all those aspects of the "art" of modelling that are easily overlooked in more theoretical expositions.   More pragmatic than ideological, the authors systematically work through the large range of "tricks" that reveal the real power of the BUGS software, for example, dealing with missing data, censoring, grouped data, prediction, ranking, parameter constraints, and so on. Many of the examples are biostatistical, but they do not require domain knowledge and are generalisable to a wide range of other application areas.   Full code and data for examples, exercises, and some solutions can be found on the book’s website.},
  pagetotal = {402},
  timestamp = {2015-03-04 22:32:21},
  publisher = {{CRC} Press},
  author = {Lunn, David and Jackson, Chris and Best, Nicky and Thomas, Andrew and Spiegelhalter, David},
  date = {2012-10-02},
  langid = {english},
  keywords = {Computers / Mathematical \& Statistical Software,Mathematics / Probability \& Statistics / General}
}

@article{Pfeffer2009,
  title = {Figaro: An object-oriented probabilistic programming language},
  timestamp = {2015-03-25 19:56:56},
  journaltitle = {Charles River Analytics Technical Report},
  shortjournal = {Charles River Anal. Tech. Rep.},
  author = {Pfeffer, Avi},
  date = {2009},
  pages = {137}
}

@article{Lebreton1992,
  title = {Modeling Survival and Testing Biological Hypotheses Using Marked Animals: A Unified Approach with Case Studies},
  volume = {62},
  rights = {Copyright © 1992 Ecological Society of America},
  issn = {0012-9615},
  doi = {10.2307/2937171},
  shorttitle = {Modeling Survival and Testing Biological Hypotheses Using Marked Animals},
  abstract = {The understanding of the dynamics of animal populations and of related ecological and evolutionary issues frequently depends on a direct analysis of life history parameters. For instance, examination of trade-offs between reproduction and survival usually rely on individually marked animals, for which the exact time of death is most often unknown, because marked individuals cannot be followed closely through time. Thus, the quantitative analysis of survival studies and experiments must be based on capture-recapture (or resighting) models which consider, besides the parameters of primary interest, recapture or resighting rates that are nuisance parameters. Capture-recapture models oriented to estimation of survival rates are the result of a recent change in emphasis from earlier approaches in which population size was the most important parameter, survival rates having been first introduced as nuisance parameters. This emphasis on survival rates in capture-recapture models developed rapidly in the 1980s and used as a basic structure the Cormack-Jolly-Seber survival model applied to an homogeneous group of animals, with various kinds of constraints on the model parameters. These approaches are conditional on first captures; hence they do not attempt to model the initial capture of unmarked animals as functions of population abundance in addition to survival and capture probabilities. This paper synthesizes, using a common framework, these recent developments together with new ones, with an emphasis on flexibility in modeling, model selection, and the analysis of multiple data sets. The effects on survival and capture rates of time, age, and categorical variables characterizing the individuals (e.g., sex) can be considered, as well as interactions between such effects. This @'analysis of variance@' philosophy emphasizes the structure of the survival and capture process rather than the technical characteristics of any particular model. The flexible array of models encompassed in this synthesis uses a common notation. As a result of the great level of flexibility and relevance achieved, the focus is changed from fitting a particular model to model building and model selection. The following procedure is recommended: (1) start from a global model compatible with the biology of the species studied and with the design of the study, and assess its fit; (2) select a more parsimonious model using Akaike's Information Criterion to limit the number of formal tests; (3) test for the most important biological questions by comparing this model with neighboring ones using likelihood ratio tests; and (4) obtain maximum likelihood estimates of model parameters with estimates of precision. Computer software is critical, as few of the models now available have parameter estimators that are in closed form. A comprehensive table of existing computer software is provided. We used RELEASE for data summary and goodness-of-fit tests and SURGE for iterative model fitting and the computation of likelihood ratio tests. Five increasingly complex examples are given to illustrate the theory. The first, using two data sets on the European Dipper (Cinclus cinclus), tests for sex-specific parameters, explores a model with time-dependent survival rates, and finally uses a priori information to model survival allowing for an environmental variable. The second uses data on two colonies of the Swift (Apus apus), and shows how interaction terms can be modeled and assessed and how survival and recapture rates sometimes partly counterbalance each other. The third shows complex variation in survival rates across sexes and age classes in the roe deer (Capreolus capreolus), with a test of density dependence in annual survival rates. The fourth is an example of experimental density manipulation using the common lizard (Lacerta vivipara). The last example attempts to examine a large and complex data set on the Greater Flamingo (Phoenicopterus ruber), where parameters are age specific, survival is a function of an environmental variable, and an age x year interaction term is important. Heterogeneity seems present in this example and cannot be adequately modeled with existing theory. The discussion presents a summary of the paradigm we recommend and details issues in model selection and design, and foreseeable future developments.},
  timestamp = {2014-12-19 21:36:39},
  eprinttype = {jstor},
  eprint = {2937171},
  number = {1},
  journaltitle = {Ecological Monographs},
  shortjournal = {Ecological Monographs},
  author = {Lebreton, Jean-Dominique and Burnham, Kenneth P. and Clobert, Jean and Anderson, David R.},
  date = {1992-03-01},
  pages = {67--118},
  keywords = {capture–recapture,dipper},
  file = {JSTOR Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/3FPKEC55/Lebreton et al. - 1992 - Modeling Survival and Testing Biological Hypothese.pdf:application/pdf}
}

@book{kery_bayesian_2012,
  title = {Bayesian Population Analysis Using {WinBUGS}: A Hierarchical Perspective},
  isbn = {978-0-12-387020-9},
  shorttitle = {Bayesian Population Analysis Using {WinBUGS}},
  abstract = {Bayesian statistics has exploded into biology and its sub-disciplines, such as ecology, over the past decade. The free software program WinBUGS, and its open-source sister OpenBugs, is currently the only flexible and general-purpose program available with which the average ecologist can conduct standard and non-standard Bayesian statistics. Comprehensive and richly commented examples illustrate a wide range of models that are most relevant to the research of a modern population ecologistAll WinBUGS/OpenBUGS analyses are completely integrated in software RIncludes complete documentation of all R and WinBUGS code required to conduct analyses and shows all the necessary steps from having the data in a text file out of Excel to interpreting and processing the output from WinBUGS in R},
  pagetotal = {556},
  timestamp = {2015-07-29 07:12:56},
  publisher = {Academic Press},
  author = {Kéry, Marc and Schaub, Michael},
  date = {2012},
  langid = {english},
  keywords = {Science / Life Sciences / Ecology,Science / Life Sciences / Zoology / General}
}

@article{Ihaka1996,
  title = {R: A Language for Data Analysis and Graphics},
  volume = {5},
  issn = {1061-8600},
  doi = {10.1080/10618600.1996.10474713},
  shorttitle = {R},
  abstract = {In this article we discuss our experience designing and implementing a statistical computing language. In developing this new language, we sought to combine what we felt were useful features from two existing computer languages. We feel that the new language provides advantages in the areas of portability, computational efficiency, memory management, and scoping.},
  timestamp = {2015-03-04 22:24:17},
  number = {3},
  journaltitle = {Journal of Computational and Graphical Statistics},
  shortjournal = {J. Comput. Graph. Stat.},
  author = {Ihaka, Ross and Gentleman, Robert},
  date = {1996-09-01},
  pages = {299--314},
  file = {ihaka-gentleman-96-JCGS-R.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/9H2VIVAP/ihaka-gentleman-96-JCGS-R.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/BCQJT7IK/login.html:text/html}
}

@article{Wood2005,
  title = {Teachers' Perceptions: barriers and supports to using technology in the classroom},
  volume = {5},
  issn = {1463-631X},
  url = {http://dx.doi.org/10.1080/14636310500186214},
  doi = {10.1080/14636310500186214},
  shorttitle = {Teachers' Perceptions},
  abstract = {Fifty‐four elementary and secondary school teachers participated in focus‐group discussions and completed a survey to examine barriers and supports to computer integration. Although teachers used computers at home and school, they were not wholly comfortable with the technology. Familiarity with computers predicted greater comfort with technology and greater comfort was related to greater integration in the classroom. Thematic analysis of the focus groups yielded six major themes, including issues related to: support, teachers, context and access, students, computer hardware and software problems, and external or other priorities. The discussion of computer integration also inspired substantial emotional responses on the part of teachers. Together, the survey and focus‐group findings yielded a framework for identifying individual and environmental issues which impact on computer integration.},
  timestamp = {2015-07-10 18:53:32},
  number = {2},
  journaltitle = {Education, Communication \& Information},
  shortjournal = {Educ. Commun. Inf.},
  author = {Wood, Eileen and Mueller, Julie and Willoughby, Teena and Specht, Jacqueline and Deyoung, Ted},
  urldate = {2015-07-10},
  date = {2005-01-01},
  pages = {183--206},
  file = {Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/223B3ZBR/login.html:text/html},
  year = 2005
}

@article{Tierney1999,
  title = {Some adaptive monte carlo methods for Bayesian inference},
  volume = {18},
  issn = {0277-6715},
  abstract = {Monte Carlo methods, in particular Markov chain Monte Carlo methods, have become increasingly important as a tool for practical Bayesian inference in recent years. A wide range of algorithms is available, and choosing an algorithm that will work well on a specific problem is challenging. It is therefore important to explore the possibility of developing adaptive strategies that choose and adjust the algorithm to a particular context based on information obtained during sampling as well as information provided with the problem. This paper outlines some of the issues in developing adaptive methods and presents some preliminary results.},
  timestamp = {2014-12-19 21:36:53},
  number = {17-18},
  journaltitle = {Statistics in Medicine},
  shortjournal = {Stat Med},
  author = {Tierney, L. and Mira, A.},
  date = {1999-09-15},
  pages = {2507--2515},
  keywords = {Algorithms,Bayes Theorem,Cardiotonic Agents,Heart Failure,Markov Chains,Monte Carlo Method},
  file = {Tierney and Mira, 1999.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/UE9AIEAM/Tierney and Mira, 1999.pdf:application/pdf},
  eprinttype = {pmid},
  eprint = {10474156}
}

@online{entwistle_impact_1992,
  title = {The impact of teaching on learning outcomes in higher education  A literature review},
  timestamp = {2015-07-07 18:49:28},
  author = {Entwistle, N.},
  date = {1992},
  langid = {english},
  keywords = {05P - Education, training},
  file = {The impact of teaching on learning outcomes in higher education A literature review - OpenGrey:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/926HTARZ/482948.html:text/html}
}

@article{Xu2005,
  title = {Survey of clustering algorithms},
  volume = {16},
  issn = {1045-9227},
  doi = {10.1109/TNN.2005.845141},
  abstract = {Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.},
  timestamp = {2014-12-19 21:36:57},
  number = {3},
  journaltitle = {{IEEE} Transactions on Neural Networks},
  shortjournal = {{IEEE} Trans. Neural Netw.},
  author = {Xu, Rui and Wunsch, {II}, D.},
  date = {2005-05},
  pages = {645--678},
  keywords = {Adaptive resonance theory (ART),Algorithms,Application software,benchmark data sets,bioinformatics,Cluster analysis,clustering,clustering algorithm,Clustering algorithms,cluster validation,Computer science,Computer Simulation,data analysis,Humans,Machine learning,Machine learning algorithms,Models, Statistical,neural networks,Neural Networks (Computer),Numerical Analysis, Computer-Assisted,pattern classification,pattern clustering,Pattern Recognition, Automated,proximity,self-organizing feature map (SOFM),Signal Processing, Computer-Assisted,Statistics,Stochastic Processes,traveling salesman problem,Traveling salesman problems},
  file = {IEEE Xplore Abstract Record:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/A2RFE97A/abs_all.html:text/html;IEEE Xplore Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/9N99WSFB/Xu and Wunsch - 2005 - Survey of clustering algorithms.pdf:application/pdf}
}

@article{Ogawa1995,
  title = {Science education in a multiscience perspective},
  volume = {79},
  timestamp = {2015-07-07 18:42:26},
  number = {5},
  journaltitle = {Science Education},
  shortjournal = {Sci. Educ.},
  author = {Ogawa, Masakata},
  date = {1995},
  pages = {583--593},
  year = 1995
}

@article{Besag1995,
  title = {Bayesian Computation and Stochastic Systems},
  volume = {10},
  rights = {Copyright © 1995 Institute of Mathematical Statistics},
  abstract = {Markov chain Monte Carlo (MCMC) methods have been used extensively in statistical physics over the last 40 years, in spatial statistics for the past 20 and in Bayesian image analysis over the last decade. In the last five years, MCMC has been introduced into significance testing, general Bayesian inference and maximum likelihood estimation. This paper presents basic methodology of MCMC, emphasizing the Bayesian paradigm, conditional probability and the intimate relationship with Markov random fields in spatial statistics. Hastings algorithms are discussed, including Gibbs, Metropolis and some other variations. Pairwise difference priors are described and are used subsequently in three Bayesian applications, in each of which there is a pronounced spatial or temporal aspect to the modeling. The examples involve logistic regression in the presence of unobserved covariates and ordinal factors; the analysis of agricultural field experiments, with adjustment for fertility gradients; and processing of low-resolution medical images obtained by a gamma camera. Additional methodological issues arise in each of these applications and in the Appendices. The paper lays particular emphasis on the calculation of posterior probabilities and concurs with others in its view that MCMC facilitates a fundamental breakthrough in applied Bayesian modeling.},
  timestamp = {2015-03-04 22:29:43},
  number = {1},
  journaltitle = {Statistical Science},
  shortjournal = {Stat. Sci.},
  author = {Besag, Julian and Green, Peter and Higdon, David and Mengersen, Kerrie},
  date = {1995},
  pages = {pp. 3--41},
  langid = {english},
  file = {Besag et al, 1995.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/NBVUETRV/Besag et al, 1995.pdf:application/pdf}
}

@article{Lele2010,
  title = {Estimability and Likelihood Inference for Generalized Linear Mixed Models Using Data Cloning},
  volume = {105},
  issn = {0162-1459},
  url = {http://amstat.tandfonline.com/doi/abs/10.1198/jasa.2010.tm09757},
  doi = {10.1198/jasa.2010.tm09757},
  timestamp = {2014-12-19 21:36:21},
  number = {492},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  author = {Lele, Subhash R. and Nadeem, Khurram and Schmuland, Byron},
  urldate = {2014-12-16},
  date = {2010-12-01},
  pages = {1617--1625},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/SB75II36/Lele et al. - 2010 - Estimability and Likelihood Inference for Generali.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/K8CUDG6X/jasa.2010.html:text/html}
}

@book{Brooks2011,
  title = {Handbook of Markov Chain Monte Carlo},
  abstract = {Since their popularization in the 1990s, Markov chain Monte Carlo (MCMC) methods have revolutionized statistical computing and have had an especially profound impact on the practice of Bayesian statistics. Furthermore, MCMC methods have enabled the development and use of intricate models in an astonishing array of disciplines as diverse as fisheries science and economics. The wide-ranging practical importance of MCMC has sparked an expansive and deep investigation into fundamental Markov chain theory.  The Handbook of Markov Chain Monte Carlo provides a reference for the broad audience of developers and users of MCMC methodology interested in keeping up with cutting-edge theory and applications. The first half of the book covers MCMC foundations, methodology, and algorithms. The second half considers the use of MCMC in a variety of practical applications including in educational research, astrophysics, brain imaging, ecology, and sociology.  The in-depth introductory section of the book allows graduate students and practicing scientists new to MCMC to become thoroughly acquainted with the basic theory, algorithms, and applications. The book supplies detailed examples and case studies of realistic scientific problems presenting the diversity of methods used by the wide-ranging MCMC community. Those familiar with MCMC methods will find this book a useful refresher of current theory and recent developments.},
  pagetotal = {620},
  timestamp = {2015-03-04 22:30:09},
  publisher = {{CRC} Press},
  author = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
  date = {2011-05-10},
  langid = {english},
  keywords = {Computers / Mathematical \& Statistical Software,Mathematics / Probability \& Statistics / General,Science / Life Sciences / Biology}
}

@article{Roberts1997,
  title = {Updating Schemes, Correlation Structure, Blocking and Parameterization for the Gibbs Sampler},
  volume = {59},
  rights = {1997 Royal Statistical Society},
  issn = {1467-9868},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/1467-9868.00070/abstract},
  doi = {10.1111/1467-9868.00070},
  abstract = {In this paper many convergence issues concerning the implementation of the Gibbs sampler are investigated. Exact computable rates of convergence for Gaussian target distributions are obtained. Different random and non-random updating strategies and blocking combinations are compared using the rates. The effect of dimensionality and correlation structure on the convergence rates are studied. Some examples are considered to demonstrate the results. For a Gaussian image analysis problem several updating strategies are described and compared. For problems in Bayesian linear models several possible parameterizations are analysed in terms of their convergence rates characterizing the optimal choice.},
  timestamp = {2014-12-19 21:37:03},
  number = {2},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  shortjournal = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  author = {Roberts, G. O. and Sahu, S. K.},
  urldate = {2014-11-19},
  date = {1997-01-01},
  pages = {291--317},
  langid = {english},
  keywords = {Bayesian inference,Blocking,Correlation Structure,Gaussian Distribution,Gibbs sampler,Markov chain Monte Carlo method,Markov Random Field,Parameterization,Random Scan,Rates of Convergence,Stochastic Relaxation,Updating Schemes},
  file = {Roberts Sahu 1997.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/R67IQ2PG/Roberts Sahu 1997.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/P9WAZ6AJ/abstract.html:text/html}
}

@article{Efron1979,
  title = {Bootstrap methods: another look at the jackknife},
  timestamp = {2014-12-19 21:36:10},
  journaltitle = {The annals of Statistics},
  shortjournal = {Ann. Stat.},
  author = {Efron, Bradley},
  date = {1979},
  pages = {1--26}
}

@article{Tierney1994,
  title = {Markov Chains for Exploring Posterior Distributions},
  volume = {22},
  rights = {Copyright © 1994 Institute of Mathematical Statistics},
  issn = {0090-5364},
  abstract = {Several Markov chain methods are available for sampling from a posterior distribution. Two important examples are the Gibbs sampler and the Metropolis algorithm. In addition, several strategies are available for constructing hybrid algorithms. This paper outlines some of the basic methods and strategies and discusses some related theoretical and practical issues. On the theoretical side, results from the theory of general state space Markov chains can be used to obtain convergence rates, laws of large numbers and central limit theorems for estimates obtained from Markov chain methods. These theoretical results can be used to guide the construction of more efficient algorithms. For the practical use of Markov chain methods, standard simulation methodology provides several variance reduction techniques and also give guidance on the choice of sample size and allocation.},
  timestamp = {2014-12-19 21:36:35},
  eprinttype = {jstor},
  eprint = {2242477},
  number = {4},
  journaltitle = {The Annals of Statistics},
  shortjournal = {The Annals of Statistics},
  author = {Tierney, Luke},
  date = {1994-12-01},
  pages = {1701--1728},
  file = {JSTOR Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/MNH6S5CX/Tierney - 1994 - Markov Chains for Exploring Posterior Distribution.pdf:application/pdf}
}

@book{eisenberg2000bridging,
  title = {Bridging disciplines in the brain, behavioral, and clinical sciences},
  timestamp = {2015-08-05 02:48:47},
  publisher = {National Academies Press},
  author = {Eisenberg, Leon and Pellmar, Terry C},
  date = {2000},
  year = 2000
}

@article{Sargent2000,
  title = {Structured Markov Chain Monte Carlo},
  volume = {9},
  issn = {1061-8600},
  url = {http://www.tandfonline.com/doi/abs/10.1080/10618600.2000.10474877},
  doi = {10.1080/10618600.2000.10474877},
  abstract = {This article introduces a general method for Bayesian computing in richly parameterized models, structured Markov chain Monte Carlo (SMCMC), that is based on a blocked hybrid of the Gibbs sampling and Metropolis—Hastings algorithms. SMCMC speeds algorithm convergence by using the structure that is present in the problem to suggest an appropriate Metropolis—Hastings candidate distribution. Although the approach is easiest to describe for hierarchical normal linear models, we show that its extension to both nonnormal and nonlinear cases is straightforward. After describing the method in detail we compare its performance (in terms of run time and autocorrelation in the samples) to other existing methods, including the single-site updating Gibbs sampler available in the popular BUGS software package. Our results suggest significant improvements in convergence for many problems using SMCMC, as well as broad applicability of the method, including previously intractable hierarchical nonlinear model settings.},
  timestamp = {2014-12-19 21:36:56},
  number = {2},
  journaltitle = {Journal of Computational and Graphical Statistics},
  shortjournal = {J. Comput. Graph. Stat.},
  author = {Sargent, Daniel J. and Hodges, James S. and Carlin, Bradley P.},
  urldate = {2014-12-16},
  date = {2000-06-01},
  pages = {217--234},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/476UBB29/Sargent et al. - 2000 - Structured Markov Chain Monte Carlo.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/JFDNE8G5/10618600.2000.html:text/html}
}

@article{Jonsen2003,
  title = {Meta-analysis of animal movement using state-space models},
  volume = {84},
  abstract = {The study of animal movement and behavior is being revolutionized by technology, such as satellite tags and harmonic radar, that allows us to track the movements of individual animals. However, our ability to analyze and model such data has lagged behind the sophisticated collection methods. We review problems with current methods and suggest a more powerful and flexible approach, state-space modeling, and we illustrate how these models can be posed in a meta-analytic framework so that information from individual trajectories may be combined optimally. State-space models enable us to deal with the complexity of modeling animals interacting with their environment but, unlike other methods, they allow simultaneous estimation of measurement error and process noise that are inherent in animal-trajectory data. A Bayesian framework allows us to incorporate important prior information when available and also allows meta-analytic techniques to be incorporated in a straightforward fashion. Meta-analysis enables both individual and broader-level inference from observations of multiple individual pathways. Our approach is powerful because it allows researchers to test hypotheses regarding animal movement, to connect theoretical models to data, and to use modern likelihood-based estimation techniques, all under a single statistical framework.},
  timestamp = {2015-03-25 23:38:53},
  number = {11},
  journaltitle = {Ecology},
  shortjournal = {Ecology},
  author = {Jonsen, Ian D. and Myers, Ransom A. and Flemming, Joanna Mills},
  date = {2003-11-01},
  pages = {3055--3063},
  keywords = {animal movement, analysis of tracking data,Bayesian models, hierarchical,behavior,dispersal,MCMC (Markov chain Monte Carlo) methods,measurement error,meta-analysis of animal movement,migration,process noise,state-space models,WinBUGs analysis},
  file = {Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/PAXU5HQT/login.html:text/html}
}

@article{Freedman1981,
  title = {Bootstrapping Regression Models},
  volume = {9},
  issn = {0090-5364, 2168-8966},
  url = {http://projecteuclid.org/euclid.aos/1176345638},
  doi = {10.1214/aos/1176345638},
  abstract = {The regression and correlation models are considered. It is shown that the bootstrap approximation to the distribution of the least squares estimates is valid, and some error bounds are given.},
  timestamp = {2014-12-19 21:36:13},
  number = {6},
  journaltitle = {The Annals of Statistics},
  shortjournal = {Ann. Statist.},
  author = {Freedman, D. A.},
  urldate = {2014-12-16},
  date = {1981-11},
  pages = {1218--1228},
  langid = {english},
  keywords = {bootstrap,correlation,least squares,Regression,Wasserstein metrics},
  file = {Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/WR8KHK65/1176345638.html:text/html},
  mrnumber = {MR630104},
  zmnumber = {0449.62046}
}

@article{pitt_filtering_1999,
  title = {Filtering via simulation: Auxiliary particle filters},
  volume = {94},
  timestamp = {2015-07-21 18:28:47},
  number = {446},
  journaltitle = {Journal of the American statistical association},
  shortjournal = {J. Am. Stat. Assoc.},
  author = {Pitt, Michael K and Shephard, Neil},
  date = {1999},
  pages = {590--599}
}

@article{incorporation2005matlab,
  title = {{MATLAB} user manual version 7.1 R14},
  timestamp = {2015-08-05 18:36:08},
  journaltitle = {Math Works Incorporation, Natick, {MA}},
  shortjournal = {Math Works Inc. Natick {MA}},
  author = {{Math Works Incorporation}},
  date = {2005},
  year = 2005
}

@book{Duval1993,
  title = {Bootstrapping: A Nonparametric Approach to Statistical Inference},
  shorttitle = {Bootstrapping},
  abstract = {Bootstrapping, a computational nonparametric technique for "re-sampling," enables researchers to draw a conclusion about the characteristics of a population strictly from the existing sample rather than by making parametric assumptions about the estimator. Using real data examples from per capita personal income to median preference differences between legislative committee members and the entire legislature, Mooney and Duval discuss how to apply bootstrapping when the underlying sampling distribution of the statistics cannot be assumed normal, as well as when the sampling distribution has no analytic solution. In addition, they show the advantages and limitations of four bootstrap confidence interval methods: normal approximation, percenti},
  pagetotal = {84},
  timestamp = {2015-03-04 22:31:00},
  publisher = {{SAGE}},
  author = {Duval, Robert},
  date = {1993-08-09},
  langid = {english},
  keywords = {Social Science / Methodology,Social Science / Research}
}

@article{Steinbakk2009,
  title = {Posterior Predictive p-values in Bayesian Hierarchical Models},
  volume = {36},
  rights = {© 2009 Board of the Foundation of the Scandinavian Journal of Statistics},
  issn = {1467-9469},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-9469.2008.00630.x/abstract},
  doi = {10.1111/j.1467-9469.2008.00630.x},
  abstract = {Abstract.  The present work focuses on extensions of the posterior predictive p-value (ppp-value) for models with hierarchical structure, designed for testing assumptions made on underlying processes. The ppp-values are popular as tools for model criticism, yet their lack of a common interpretation limit their practical use. We discuss different extensions of ppp-values to hierarchical models, allowing for discrepancy measures that can be used for checking properties of the model at all stages. Through analytical derivations and simulation studies on simple models, we show that similar to the standard ppp-values, these extensions are typically far from uniformly distributed under the model assumptions and can give poor power in a hypothesis testing framework. We propose a calibration of the p-values, making the resulting calibrated p-values uniformly distributed under the model conditions. Illustrations are made through a real example of multinomial regression to age distributions of fish.},
  timestamp = {2014-12-19 21:36:48},
  number = {2},
  journaltitle = {Scandinavian Journal of Statistics},
  shortjournal = {Scand. J. Stat.},
  author = {Steinbakk, Gunnhildur Högnadóttir and Storvik, Geir Olve},
  urldate = {2014-12-16},
  date = {2009},
  pages = {320--336},
  langid = {english},
  keywords = {calibration of p-values,hierarchical models,model criticism,posterior predictive p-values},
  file = {j.1467-9469.2008.00630.x.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/CU8CRIRJ/j.1467-9469.2008.00630.x.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/8Q7H4E4J/full.html:text/html}
}

@inproceedings{canny2013bidmach,
  title = {Bidmach: Large-scale learning with zero memory allocation},
  timestamp = {2015-08-05 18:47:14},
  booktitle = {{BigLearning}, {NIPS} Workshop},
  author = {Canny, John and Zhao, Huasha},
  date = {2013},
  year = 2013
}

@article{Ely1995,
  title = {Technology Is the Answer! But What Was the Question?},
  url = {http://eric.ed.gov/?id=ED381152},
  shorttitle = {Technology Is the Answer! But What Was the Question?},
  abstract = {This paper examines how, why, and by whom technology is being used in schools. Educational technology is defined as the systematic design and use of hardware and software to achieve specific objectives. Recent studies indicate that the most frequent location of computers in schools is in the administrative office; second is in the library media center and third in a computer lab. Computers are used mostly for word processing, followed by drill and practice and educational games. The following rationales for using computers in schools are identified: social, vocational, pedagogic, and catalytic. In the United States, the social and vocational rationales are dominant. Some studies show that computer-based programs in elementary education benefit only the highest scoring students and students taught by teachers most knowledgeable about the computer system being used; in colleges and universities only about 10 percent of the faculty use technology in the classroom. Factors that},
  timestamp = {2015-07-10 18:48:45},
  author = {Ely, Donald P.},
  urldate = {2015-07-10},
  date = {1995-04-14},
  langid = {english},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/DF8FWMWR/Ely - 1995 - Technology Is the Answer! But What Was the Questio.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/CTDN3ZBE/eric.ed.gov.html:text/html},
  year = 1995
},

@article{Duane1987,
  title = {Hybrid Monte Carlo},
  volume = {195},
  issn = {0370-2693},
  abstract = {We present a new method for the numerical simulation of lattice field theory. A hybrid (molecular dynamics/Langevin) algorithm is used to guide a Monte Carlo simulation. There are no discretization errors even for large step sizes. The method is especially efficient for systems such as quantum chromodynamics which contain fermionic degrees of freedom. Detailed results are presented for four-dimensional compact quantum electrodynamics including the dynamical effects of electrons.},
  timestamp = {2015-03-04 22:30:44},
  number = {2},
  journaltitle = {Physics Letters B},
  shortjournal = {Physics Letters B},
  author = {Duane, Simon and Kennedy, A. D. and Pendleton, Brian J. and Roweth, Duncan},
  date = {1987-09-03},
  pages = {216--222},
  file = {ScienceDirect Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/FW8684E5/Duane et al. - 1987 - Hybrid Monte Carlo.pdf:application/pdf;ScienceDirect Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/Q52J94EN/037026938791197X.html:text/html}
}

@article{Baker1997,
  title = {Technology in the Classroom: From Theory to Practice},
  volume = {32},
  timestamp = {2015-07-10 18:47:28},
  number = {5},
  journaltitle = {Educom Review},
  shortjournal = {Educom Rev.},
  author = {Baker, Warren},
  date = {1997},
  year = 1997
}

@article{Gilks1994,
  title = {A Language and Program for Complex Bayesian Modelling},
  volume = {43},
  rights = {Copyright © 1994 Royal Statistical Society},
  abstract = {Gibbs sampling has enormous potential for analysing complex data sets. However, routine use of Gibbs sampling has been hampered by the lack of general purpose software for its implementation. Until now all applications have involved writing one-off computer code in low or intermediate level languages such as C or Fortran. We describe some general purpose software that we are currently developing for implementing Gibbs sampling: BUGS (Bayesian inference using Gibbs sampling). The BUGS system comprises three components: first, a natural language for specifying complex models; second, an 'expert system' for deciding appropriate methods for obtaining samples required by the Gibbs sampler: third, a sampling module containing numerical routines to perform the sampling. \$S\$ objects are used for data input and output. BUGS is written in Modula-2 and runs under both DOS and UNIX.},
  timestamp = {2015-03-25 23:29:19},
  number = {1},
  journaltitle = {Journal of the Royal Statistical Society. Series D (The Statistician)},
  shortjournal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
  author = {Gilks, W. R. and Thomas, A. and Spiegelhalter, D. J.},
  date = {1994-01-01},
  pages = {169--177}
}

@book{Robert2004,
  title = {Monte Carlo statistical methods},
  volume = {319},
  timestamp = {2015-01-28 23:29:06},
  publisher = {Citeseer},
  author = {Robert, Christian P and Casella, George},
  date = {2004}
}

@article{Roberts1996,
  title = {Exponential Convergence of Langevin Distributions and Their Discrete Approximations},
  volume = {2},
  rights = {Copyright © 1996 International Statistical Institute ({ISI}) and Bernoulli Society for Mathematical Statistics and Probability},
  issn = {1350-7265},
  doi = {10.2307/3318418},
  abstract = {In this paper we consider a continuous-time method of approximating a given distribution π using the Langevin diffusion d Lt= d Wt+1/2∇ log π ( Lt) dt. We find conditions under this diffusion converges exponentially quickly to π or does not: in one dimension, these are essentially that for distributions with exponential tails of the form π(x) ∝ exp(-γ |x|β), 0 \ensuremath{<} β \ensuremath{<} ∞, exponential convergence occurs if and only if β ≥ 1. We then consider conditions under which the discrete approximations to the diffusion converge. We first show that even when the diffusion itself converges, naive discretizations need not do so. We then consider a 'Metropolis-adjusted' version of the algorithm, and find conditions under which this also converges at an exponential rate: perhaps surprisingly, even the Metropolized version need not converge exponentially fast even if the diffusion does. We briefly discuss a truncated form of the algorithm which, in practice, should avoid the difficulties of the other forms.},
  timestamp = {2014-12-19 21:36:24},
  eprinttype = {jstor},
  eprint = {3318418},
  number = {4},
  journaltitle = {Bernoulli},
  shortjournal = {Bernoulli},
  author = {Roberts, Gareth O. and Tweedie, Richard L.},
  date = {1996-12-01},
  pages = {341--363},
  file = {JSTOR Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/IDBHSZR4/Roberts and Tweedie - 1996 - Exponential Convergence of Langevin Distributions .pdf:application/pdf}
}

@online{Guennebaud2010,
  title = {Eigen},
  url = {http://eigen.tuxfamily.org},
  timestamp = {2014-12-22 21:25:14},
  author = {Guennebaud, Gaël and Jacob, Benoit},
  date = {2010}
}

@article{Plummer2006,
  title = {{CODA}: convergence diagnosis and output analysis for {MCMC}},
  volume = {6},
  issn = {1609-3631},
  url = {http://cran.r-project.org/doc/Rnews/Rnews_2006-1.pdf#page=7},
  shorttitle = {{CODA}},
  abstract = {{[}1st paragraph] At first sight, Bayesian inference with Markov Chain Monte Carlo (MCMC) appears to be straightforward. The user defines a full probability model, perhaps using one of the programs discussed in this issue; an underlying sampling engine takes the model definition and returns a sequence of dependent samples from the posterior distribution of the model parameters, given the supplied data. The user can derive any summary of the posterior distribution from this sample. For example, to calculate a 95\% credible interval for a parameter α, it suffices to take 1000 MCMC iterations of α and sort them so that α\textsubscript{1}\ensuremath{<}α\textsubscript{2}\ensuremath{<}...\ensuremath{<}α\textsubscript{1000}. The credible interval estimate is then (α\textsubscript{25}, α\textsubscript{975}). However, there is a price to be paid for this simplicity. Unlike most numerical methods used in statistical inference, MCMC does not give a clear indication of whether it has converged. The underlying Markov chain theory only guarantees that the distribution of the output will converge to the posterior in the limit as the number of iterations increases to infinity. The user is generally ignorant about how quickly convergence occurs, and therefore has to fall back on post hoc testing of the sampled output. By convention, the sample is divided into two parts: a “burn in” period during which all samples are discarded, and the remainder of the run in which the chain is considered to have converged sufficiently close to the limiting distribution to be used. Two questions then arise: 1. How long should the burn in period be? 2. How many samples are required to accurately estimate posterior quantities of interest? The \textbf{coda} package for R contains a set of functions designed to help the user answer these questions. Some of these convergence diagnostics are simple graphical ways of summarizing the data. Others are formal statistical tests.},
  timestamp = {2014-12-19 21:36:15},
  number = {1},
  journaltitle = {R News},
  shortjournal = {R News},
  author = {Plummer, Martyn and Best, Nicky and Cowles, Kate and Vines, Karen},
  urldate = {2014-11-24},
  date = {2006-03},
  pages = {7--11},
  file = {CODA package info.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/K25CZFAG/CODA package info.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/KHB86U9Z/22547.html:text/html}
}

@report{Gelfand1992,
  title = {Model Determination Using Predictive Distributions with Implementation via Sampling-Based Methods},
  rights = {Approved for public release; distribution is unlimited.},
  abstract = {Model determination is divided into the issues of model adequacy and model selection. Predictive distributions are used to address both issues. This seems natural since, typically, prediction is a primary purpose for the chosen model. A cross-validation viewpoint is argued for. In particular, for a given model, it is proposed to validate conditional predictive distributions arising from single point deletion against observed responses. Sampling based methods are used to carry out required calculations. An example investigates the adequacy of and rather subtle choice between two sigmoidal growth models of the same dimension.},
  timestamp = {2014-12-19 21:36:38},
  author = {Gelfand, Alan E. and Dey, Dipak K. and Chang, Hong},
  date = {1992-12-04},
  langid = {english},
  keywords = {*MATHEMATICAL MODELS,*PREDICTIONS,*STATISTICAL DISTRIBUTIONS,*STATISTICAL SAMPLES,DETERMINATION,DISTRIBUTION,LOGISTICS,MODELS,RESPONSE,SAMPLING,SELECTION,Statistics and Probability,VALIDATION,WUNR042267},
  file = {ADA258777.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/AKKJCK8D/ADA258777.pdf:application/pdf;DTIC Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/2CK9J6QM/oai.html:text/html}
}

@article{baker_technology_1997,
  title = {Technology in the Classroom: From Theory to Practice},
  volume = {32},
  issn = {1045-9146},
  shorttitle = {Technology in the Classroom},
  abstract = {Despite the effectiveness of computer-mediated instructional (CMI) materials, relatively little use is made of technology in mainstream college teaching. This article examines why CMI materials are not widely used; the mediated learning model that embeds CMI materials within a computer-mediated, communication-rich instructional environment; and the application of the model to introductory and intermediate college algebra. (PEN)},
  timestamp = {2015-07-10 18:46:24},
  number = {5},
  journaltitle = {Educom Review},
  shortjournal = {Educom Rev.},
  author = {Baker, Warren},
  date = {1997-01},
  langid = {english},
  keywords = {Algebra,Classroom Environment,Computer Assisted Instruction,Computer Uses in Education,Educational Technology,Higher Education,MODELS},
  file = {Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/PFBHAGG3/eric.ed.gov.html:text/html}
}

@article{Provost2013,
  title = {Data Science and its Relationship to Big Data and Data-Driven Decision Making},
  volume = {1},
  issn = {2167-6461},
  url = {http://online.liebertpub.com/doi/abs/10.1089/big.2013.1508},
  doi = {10.1089/big.2013.1508},
  abstract = {Companies have realized they need to hire data scientists, academic institutions are scrambling to put together data-science programs, and publications are touting data science as a hot—even “sexy”—career choice. However, there is confusion about what exactly data science is, and this confusion could lead to disillusionment as the concept diffuses into meaningless buzz. In this article, we argue that there are good reasons why it has been hard to pin down exactly what is data science. One reason is that data science is intricately intertwined with other important concepts also of growing importance, such as big data and data-driven decision making. Another reason is the natural tendency to associate what a practitioner does with the definition of the practitioner's field; this can result in overlooking the fundamentals of the field. We believe that trying to define the boundaries of data science precisely is not of the utmost importance. We can debate the boundaries of the field in an academic setting, but in order for data science to serve business effectively, it is important (i) to understand its relationships to other important related concepts, and (ii) to begin to identify the fundamental principles underlying data science. Once we embrace (ii), we can much better understand and explain exactly what data science has to offer. Furthermore, only once we embrace (ii) should we be comfortable calling it data science. In this article, we present a perspective that addresses all these concepts. We close by offering, as examples, a partial list of fundamental principles underlying data science.},
  timestamp = {2015-07-07 23:08:14},
  number = {1},
  journaltitle = {Big Data},
  shortjournal = {Big Data},
  author = {Provost, Foster and Fawcett, Tom},
  urldate = {2015-07-07},
  date = {2013-02-13},
  pages = {51--59},
  file = {Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/W7ACQB7M/login.html:text/html},
  year = 2013
}

@report{Odersky2004,
  title = {An overview of the Scala programming language},
  timestamp = {2015-03-02 19:50:59},
  author = {Odersky, Martin and Altherr, Philippe and Cremet, Vincent and Emir, Burak and Maneth, Sebastian and Micheloud, Stéphane and Mihaylov, Nikolay and Schinz, Michel and Stenman, Erik and Zenger, Matthias},
  date = {2004}
}

@article{Plummer2011,
  title = {{JAGS} Version 3.1. 0 user manual},
  timestamp = {2014-12-19 21:36:32},
  journaltitle = {International Agency for Research on Cancer},
  shortjournal = {Int. Agency Res. Cancer},
  author = {Plummer, Martyn},
  date = {2011}
}

@article{Pinheiro1995,
  title = {Approximations to the Log-Likelihood Function in the Nonlinear Mixed-Effects Model},
  volume = {4},
  issn = {1061-8600},
  url = {http://www.tandfonline.com/doi/abs/10.1080/10618600.1995.10474663},
  doi = {10.1080/10618600.1995.10474663},
  abstract = {Nonlinear mixed-effects models have received a great deal of attention in the statistical literature in recent years because of the flexibility they offer in handling the unbalanced repeated-measures data that arise in different areas of investigation, such as pharmacokinetics and economics. Several different methods for estimating the parameters in nonlinear mixed-effects model have been proposed. We concentrate here on two of them—maximum likelihood and restricted maximum likelihood. A rather complex numerical issue for (restricted) maximum likelihood estimation in nonlinear mixed-effects models is the evaluation of the log-likelihood function of the data, because it involves the evaluation of a multiple integral that, in most cases, does not have a closed-form expression. We consider here four different approximations to the log-likelihood, comparing their computational and statistical properties. We conclude that the linear mixed-effects (LME) approximation suggested by Lindstrom and Bates, the Laplacian approximation, and Gaussian quadrature centered at the conditional modes of the random effects are quite accurate and computationally efficient. Gaussian quadrature centered at the expected value of the random effects is quite inaccurate for a smaller number of abscissas and computationally inefficient for a larger number of abscissas. Importance sampling is accurate, but quite inefficient computationally.},
  timestamp = {2015-01-16 23:08:39},
  number = {1},
  journaltitle = {Journal of Computational and Graphical Statistics},
  shortjournal = {J. Comput. Graph. Stat.},
  author = {Pinheiro, José C. and Bates, Douglas M.},
  urldate = {2015-01-16},
  date = {1995-03-01},
  pages = {12--35},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/QCQZG3KE/Pinheiro and Bates - 1995 - Approximations to the Log-Likelihood Function in t.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/K7BJSRX5/Pinheiro and Bates - 1995 - Approximations to the Log-Likelihood Function in t.html:text/html}
}

@book{Kelly2008,
  title = {Domain-Specific Modeling: Enabling Full Code Generation},
  isbn = {978-0-470-24925-3},
  shorttitle = {Domain-Specific Modeling},
  abstract = {"{[}The authors] are pioneers. . . . Few in our industry have their breadth of knowledge and experience." —From the Foreword by Dave Thomas, Bedarra Labs Domain-Specific Modeling (DSM) is the latest approach to software development, promising to greatly increase the speed and ease of software creation. Early adopters of DSM have been enjoying productivity increases of 500–1000\% in production for over a decade. This book introduces DSM and offers examples from various fields to illustrate to experienced developers how DSM can improve software development in their teams. Two authorities in the field explain what DSM is, why it works, and how to successfully create and use a DSM solution to improve productivity and quality. Divided into four parts, the book covers: background and motivation; fundamentals; in-depth examples; and creating DSM solutions. There is an emphasis throughout the book on practical guidelines for implementing DSM, including how to identify the necessary language constructs, how to generate full code from models, and how to provide tool support for a new DSM language. The example cases described in the book are available the book's Website, www.dsmbook.com, along with, an evaluation copy of the MetaEdit+ tool (for Windows, Mac OS X, and Linux), which allows readers to examine and try out the modeling languages and code generators. Domain-Specific Modeling is an essential reference for lead developers, software engineers, architects, methodologists, and technical managers who want to learn how to create a DSM solution and successfully put it into practice.},
  pagetotal = {446},
  timestamp = {2015-03-02 19:46:49},
  publisher = {John Wiley \& Sons},
  author = {Kelly, Steven and Tolvanen, Juha-Pekka},
  date = {2008-04-11},
  langid = {english},
  keywords = {Computers / Programming / General,Computers / Programming / Object Oriented}
}

@article{Duan2007,
  title = {Generalized Spatial Dirichlet Process Models},
  volume = {94},
  issn = {0006-3444, 1464-3510},
  url = {http://biomet.oxfordjournals.org/content/94/4/809},
  doi = {10.1093/biomet/asm071},
  abstract = {Many models for the study of point-referenced data explicitly introduce spatial random effects to capture residual spatial association. These spatial effects are customarily modelled as a zero-mean stationary Gaussian process. The spatial Dirichlet process introduced by Gelfand et al. (2005) produces a random spatial process which is neither Gaussian nor stationary. Rather, it varies about a process that is assumed to be stationary and Gaussian. The spatial Dirichlet process arises as a probability-weighted collection of random surfaces. This can be limiting for modelling and inferential purposes since it insists that a process realization must be one of these surfaces. We introduce a random distribution for the spatial effects that allows different surface selection at different sites. Moreover, we can specify the model so that the marginal distribution of the effect at each site still comes from a Dirichlet process. The development is offered constructively, providing a multivariate extension of the stick-breaking representation of the weights. We then introduce mixing using this generalized spatial Dirichlet process. We illustrate with a simulated dataset of independent replications and note that we can embed the generalized process within a dynamic model specification to eliminate the independence assumption.},
  timestamp = {2015-03-08 03:30:33},
  number = {4},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  author = {Duan, Jason A. and Guindani, Michele and Gelfand, Alan E.},
  urldate = {2015-03-08},
  date = {2007-12-01},
  pages = {809--825},
  langid = {english},
  keywords = {Dirichlet process mixing,Dynamic model,Latent process,Non-Gaussian,Nonstationary,Stick breaking},
  file = {Biometrika-2007-Duan-809-25.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/4MC6JJW3/Biometrika-2007-Duan-809-25.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/9BZRVKTR/login.html:text/html}
}

@book{Rudy1984,
  title = {The universities of Europe, 1100-1914: a history},
  timestamp = {2015-07-10 18:33:53},
  publisher = {Fairleigh Dickinson University Press},
  author = {Rudy, Willis},
  date = {1984},
  year = 1994
}

@article{Grant2000,
  title = {{DyC}: an expressive annotation-directed dynamic compiler for C},
  volume = {248},
  issn = {0304-3975},
  doi = {10.1016/S0304-3975(00)00051-7},
  shorttitle = {{DyC}},
  abstract = {We present the design of DyC, a dynamic-compilation system for C based on run-time specialization. Directed by a few declarative user annotations that specify the variables and code on which dynamic compilation should take place, a binding-time analysis computes the set of run-time constants at each program point in the annotated procedure's control-flow graph; the analysis supports program-point-specific polyvariant division and specialization. The results of the analysis guide the construction of a run-time specializer for each dynamically compiled region; the specializer supports various caching strategies for managing dynamically generated code and mixes of speculative and demand-driven specialization of dynamic branch successors. Most of the key cost/benefit trade-offs in the binding-time analysis and the run-time specializer are open to user control through declarative policy annotations.

DyC has been implemented in the context of an optimizing compiler, and initial results have been promising. The speedups we have obtained are good, and the dynamic-compilation overhead is among the lowest of any dynamic-compilation system, typically 20–200 cycles per instruction generated on a Digital Alpha 21164. The majority of DyC's functionality has been used to dynamically compile an instruction-set simulator. Only three annotations were required, but a few other changes to the program had to be made due to DyC's lack of support for static global variables. This deficiency and DyC's rudimentary support for partially static data structures are the primary obstacles to making DyC easy to use.},
  timestamp = {2015-03-04 22:24:10},
  number = {1–2},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  series = {{PEPM}'97},
  author = {Grant, Brian and Mock, Markus and Philipose, Matthai and Chambers, Craig and Eggers, Susan J.},
  date = {2000-10-06},
  pages = {147--199},
  keywords = {C language,Constant folding,Dataflow analysis,Dynamic compilation,partial evaluation,Program optimization,Run-time code generation,Specialization},
  file = {1-s2.0-S0304397500000517-main.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/W8BC5XGC/1-s2.0-S0304397500000517-main.pdf:application/pdf;ScienceDirect Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/2XN57EKF/login.html:text/html}
}

@article{Roberts1998a,
  title = {Optimal scaling of discrete approximations to Langevin diffusions},
  volume = {60},
  rights = {1998 Royal Statistical Society},
  issn = {1467-9868},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/1467-9868.00123/abstract},
  doi = {10.1111/1467-9868.00123},
  abstract = {We consider the optimal scaling problem for proposal distributions in Hastings–Metropolis algorithms derived from Langevin diffusions. We prove an asymptotic diffusion limit theorem and show that the relative efficiency of the algorithm can be characterized by its overall acceptance rate, independently of the target distribution. The asymptotically optimal acceptance rate is 0.574. We show that, as a function of dimension n, the complexity of the algorithm is O(n1/3), which compares favourably with the O(n) complexity of random walk Metropolis algorithms. We illustrate this comparison with some example simulations.},
  timestamp = {2014-12-19 21:36:45},
  number = {1},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  shortjournal = {J. R. Stat. Soc. Ser. B Stat. Methodol.},
  author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
  urldate = {2014-12-16},
  date = {1998},
  pages = {255--268},
  langid = {english},
  keywords = {Hastings–Metropolis algorithm,Langevin algorithm,Markov chain Monte Carlo method,weak convergence},
  file = {Roberts and Rosenthal, 1998.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/X3HXEGCB/Roberts and Rosenthal, 1998.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/6WDH9AZM/abstract.html:text/html}
}

@online{plotnikoff_classes_2013,
  title = {Classes should do hands-on exercises before reading and video, Stanford researchers say},
  url = {http://news.stanford.edu/news/2013/july/flipped-learning-model-071613.html},
  abstract = {A study of how students best learned a neuroscience lesson showed a distinct benefit to starting out by working with an interactive 3D model of the brain.},
  timestamp = {2015-08-05 02:47:25},
  titleaddon = {Stanford University},
  author = {Plotnikoff, David},
  date = {2013-07-16},
  year = 2013
}

@article{Roberts1998,
  title = {Optimal metropolis algorithms for product measures on the vertices of a hypercube},
  volume = {62},
  issn = {1045-1129},
  url = {http://www.tandfonline.com/doi/abs/10.1080/17442509808834136},
  doi = {10.1080/17442509808834136},
  abstract = {Optimal scaling problems for high dimensional Metropolis-Hastings algorithms can often be solved by means of diffusion approximation results. These solutions are particularly appealing since they can often be characterised in terms of a simple, observable property of the Markov chain sample path, namely the overall proportion of accepted iterations for the chain. For discrete state space problems, analogous scaling problems can be defined, though due to discrete effects, a simple characterisation of the asymptotically optimal solution is not available. This paper considers the simplest possible (and most discrete) example of such a problem, demonstrating that, at least for sufficiently 'smooth' distributions in high dimensional problems,the Metropolis algorithm behaves similarly to its counterpart on the continuous state space},
  timestamp = {2014-12-19 21:36:42},
  number = {3-4},
  journaltitle = {Stochastics and Stochastic Reports},
  shortjournal = {Stoch. Stoch. Rep.},
  author = {Roberts, Gareth O.},
  urldate = {2014-12-03},
  date = {1998-01-01},
  pages = {275--283},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/4JUAW5DM/Roberts - 1998 - Optimal metropolis algorithms for product measures.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/2PZGJQXA/17442509808834136.html:text/html}
}

@article{StanDevelopmentTeam2014,
  title = {Stan: A C++ Library for Probability and Sampling, Version 2.5.0},
  url = {http://mc-stan.org/},
  timestamp = {2015-01-09 00:08:55},
  author = {Stan Development Team, .},
  date = {2014}
}

@article{Mengersen1996,
  title = {Rates of convergence of the Hastings and Metropolis algorithms},
  volume = {24},
  issn = {0090-5364, 2168-8966},
  url = {http://projecteuclid.org/euclid.aos/1033066201},
  doi = {10.1214/aos/1033066201},
  abstract = {We apply recent results in Markov chain theory to Hastings and Metropolis algorithms with either independent or symmetric candidate distributions, and provide necessary and sufficient conditions for the algorithms to converge at a geometric rate to a prescribed distribution π\ensuremath{\backslash}pi. In the independence case (in Rk\ensuremath{\backslash}mathbb\{R\}\^{}k) these indicate that geometric convergence essentially occurs if and only if the candidate density is bounded below by a multiple of π\ensuremath{\backslash}pi; in the symmetric case (in R\ensuremath{\backslash}mathbb\{R\} only) we show geometric convergence essentially occurs if and only if π\ensuremath{\backslash}pi has geometric tails. We also evaluate recently developed computable bounds on the rates of convergence in this context: examples show that these theoretical bounds can be inherently extremely conservative, although when the chain is stochastically monotone the bounds may well be effective.},
  timestamp = {2014-12-19 21:36:51},
  number = {1},
  journaltitle = {The Annals of Statistics},
  shortjournal = {Ann. Statist.},
  author = {Mengersen, K. L. and Tweedie, R. L.},
  urldate = {2014-12-17},
  date = {1996-02},
  pages = {101--121},
  keywords = {geometric ergodicity,Gibbs sampling,Hastings algorithms,irreducible Markov processes,log-concave distributions,Markov Chain Monte Carlo,Metropolis algorithms,Posterior distributions,stochastic monotonicity},
  file = {Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/4G3BI85N/1033066201.html:text/html},
  mrnumber = {MR1389882},
  zmnumber = {0854.60065}
}

@article{Roberts1996a,
  title = {Geometric convergence and central limit theorems for multidimensional Hastings and Metropolis algorithms},
  volume = {83},
  issn = {0006-3444, 1464-3510},
  url = {http://biomet.oxfordjournals.org/content/83/1/95},
  doi = {10.1093/biomet/83.1.95},
  abstract = {We develop results on geometric ergodicity of Markov chains and apply these and other recent results in Markov chain theory to multidimensional Hastings and Metropolis algorithms. For those based on random walk candidate distributions, we find sufficient conditions for moments and moment generating functions to converge at a geometric rate to a prescribed distribution π. By phrasing the conditions in terms of the curvature of the densities we show that the results apply to all distributions with positive densities in a large class which encompasses many commonly-used statistical forms. From these results we develop central limit theorems for the Metropolis algorithm. Converse results, showing non-geometric convergence rates for chains where the rejection rate is not bounded away from unity, are also given; these show that the negative-definiteness property is not redundant.},
  timestamp = {2014-12-19 21:36:26},
  number = {1},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  author = {Roberts, G. O. and Tweedie, R. L.},
  urldate = {2014-12-17},
  date = {1996-03-01},
  pages = {95--110},
  langid = {english},
  keywords = {geometric ergodicity,Gibbs sampling Hastings algorithm,Irreducible Markov process,Markov Chain Monte Carlo,Metropolis algorithm,Posterior distribution},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/TZADPE8M/Roberts and Tweedie - 1996 - Geometric convergence and central limit theorems f.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/BVMS5PGX/95.html:text/html}
}

@article{Efron1986,
  title = {Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy},
  timestamp = {2014-12-19 21:36:11},
  journaltitle = {Statistical science},
  shortjournal = {Stat. Sci.},
  author = {Efron, Bradley and Tibshirani, Robert},
  date = {1986},
  pages = {54--75}
}

@article{Topping1996,
  title = {The effectiveness of peer tutoring in further and higher education: A typology and review of the literature},
  volume = {32},
  issn = {0018-1560, 1573-174X},
  url = {http://link.springer.com/article/10.1007/BF00138870},
  doi = {10.1007/BF00138870},
  shorttitle = {The effectiveness of peer tutoring in further and higher education},
  abstract = {Quality, outcomes and cost-effectiveness of methods of teaching and learning in colleges and universities are being scrutinised more closely. The increasing use of peer tutoring in this context necessitates a clear definition and typology, which are outlined. The theoretical advantages of peer tutoring are discussed and the research on peer tutoring in schools briefly considered. The substantial existing research on the effectiveness of the many different types and formats of peer tutoring within colleges and universities is then reviewed. Much is already known about the effectiveness of some types of peer tutoring and this merits wider dissemination to practitioners. Directions for future research are indicated.},
  timestamp = {2015-07-07 18:52:44},
  number = {3},
  journaltitle = {Higher Education},
  shortjournal = {High Educ},
  author = {Topping, K. J.},
  urldate = {2015-07-07},
  date = {1996-10},
  pages = {321--345},
  langid = {english},
  keywords = {Education (general),Psychology, general},
  file = {Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/6X5NWCX7/login.html:text/html},
  year = 1996
}

@article{Amit1991,
  title = {Comparing sweep strategies for stochastic relaxation},
  volume = {37},
  issn = {0047-259X},
  doi = {10.1016/0047-259X(91)90080-L},
  abstract = {The rate of convergence of various sweep strategies of stochastic relaxation for simulating multivariate Gaussian measures are calculated and compared. Each sweep strategy prescribes a method for chosing which coordinates of the random vector are to be updated. Deterministic sweep strategies in which the coordinates are updated according to a fixed order are compared to random strategies in which the coordinate to be updated is chosen through some random mechanism. In addition block updating, in which a few coordinates are updated simultaneously, is compared to single coordinate updating.},
  timestamp = {2015-03-04 22:29:07},
  number = {2},
  journaltitle = {Journal of Multivariate Analysis},
  shortjournal = {Journal of Multivariate Analysis},
  author = {Amit, Y and Grenander, U},
  date = {1991-05},
  pages = {197--222},
  keywords = {products of random affine maps,Rates of Convergence,Stochastic Relaxation,sweep strategies},
  file = {ScienceDirect Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/E78SNK53/Amit and Grenander - 1991 - Comparing sweep strategies for stochastic relaxati.pdf:application/pdf;ScienceDirect Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/XBNKVAGE/0047259X9190080L.html:text/html}
}

@article{todeschini_biips_2014,
  title = {Biips: Software for Bayesian Inference with Interacting Particle Systems},
  shorttitle = {Biips},
  abstract = {Biips is a software platform for automatic Bayesian inference with interacting particle systems. Biips allows users to define their statistical model in the probabilistic programming BUGS language, as well as to add custom functions or samplers within this language. Then it runs sequential Monte Carlo based algorithms (particle filters, particle independent Metropolis-Hastings, particle marginal Metropolis-Hastings) in a black-box manner so that to approximate the posterior distribution of interest as well as the marginal likelihood. The software is developed in C++ with interfaces with the softwares R, Matlab and Octave.},
  timestamp = {2015-07-21 18:22:28},
  eprinttype = {arxiv},
  eprint = {1412.3779},
  author = {Todeschini, Adrien and Caron, François and Fuentes, Marc and Legrand, Pierrick and Del Moral, Pierre},
  date = {2014-12-11},
  keywords = {Statistics - Computation},
  file = {arXiv\:1412.3779 PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/6MXDPTJR/Todeschini et al. - 2014 - Biips Software for Bayesian Inference with Intera.pdf:application/pdf;arXiv.org Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/BI4QJZQ4/Todeschini et al. - 2014 - Biips Software for Bayesian Inference with Intera.html:text/html}
}

@article{Metropolis1953,
  title = {Equation of State Calculations by Fast Computing Machines},
  volume = {21},
  issn = {0021-9606, 1089-7690},
  url = {http://scitation.aip.org/content/aip/journal/jcp/21/6/10.1063/1.1699114},
  doi = {10.1063/1.1699114},
  abstract = {A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two‐dimensional rigid‐sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four‐term virial coefficient expansion.},
  timestamp = {2014-12-19 21:36:20},
  number = {6},
  journaltitle = {The Journal of Chemical Physics},
  shortjournal = {J. Chem. Phys.},
  author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
  urldate = {2014-11-19},
  date = {1953-06-01},
  pages = {1087--1092},
  keywords = {Atomic and molecular interactions,Equations of state,Monte Carlo methods},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/764EKUW7/Metropolis et al. - 1953 - Equation of State Calculations by Fast Computing M.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/URIW2TQP/1.html:text/html}
}

@book{beard2010experiential,
  title = {The experiential learning toolkit: Blending practice with concepts},
  timestamp = {2015-08-06 02:17:42},
  publisher = {Kogan Page Publishers},
  author = {Beard, Colin},
  date = {2010},
  year = 2010
}

@book{stata2005stata,
  title = {Stata Statistical Software Release 9},
  timestamp = {2015-08-05 18:31:49},
  publisher = {Stata Press Publication},
  author = {{Stata Corporation}},
  date = {2005},
  year = 2005
}

@article{Zipunnikov2006,
  title = {Monte Carlo {EM} for generalized linear mixed models using randomized spherical radial integration},
  timestamp = {2015-01-16 23:18:37},
  author = {Zipunnikov, Vadim V and Booth, James G},
  date = {2006}
}

@article{Lele2007,
  title = {Data cloning: easy maximum likelihood estimation for complex ecological models using Bayesian Markov chain Monte Carlo methods},
  volume = {10},
  rights = {No claim to original {US} government works},
  issn = {1461-0248},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1461-0248.2007.01047.x/abstract},
  doi = {10.1111/j.1461-0248.2007.01047.x},
  shorttitle = {Data cloning},
  abstract = {We introduce a new statistical computing method, called data cloning, to calculate maximum likelihood estimates and their standard errors for complex ecological models. Although the method uses the Bayesian framework and exploits the computational simplicity of the Markov chain Monte Carlo (MCMC) algorithms, it provides valid frequentist inferences such as the maximum likelihood estimates and their standard errors. The inferences are completely invariant to the choice of the prior distributions and therefore avoid the inherent subjectivity of the Bayesian approach. The data cloning method is easily implemented using standard MCMC software. Data cloning is particularly useful for analysing ecological situations in which hierarchical statistical models, such as state-space models and mixed effects models, are appropriate. We illustrate the method by fitting two nonlinear population dynamics models to data in the presence of process and observation noise.},
  timestamp = {2014-12-19 21:36:19},
  number = {7},
  journaltitle = {Ecology Letters},
  shortjournal = {Ecol. Lett.},
  author = {Lele, Subhash R. and Dennis, Brian and Lutscher, Frithjof},
  urldate = {2014-12-16},
  date = {2007},
  pages = {551--563},
  langid = {english},
  keywords = {Bayesian statistics,density dependence,Fisher information,frequentist statistics,generalized linear mixed models,hierarchical models,Markov Chain Monte Carlo,state-space models,stochastic population models},
  file = {Lele, Dennis, Lutscher, 2007.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/GRPMBM58/Lele, Dennis, Lutscher, 2007.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/H5UT4XHH/full.html:text/html}
}

@article{Turek2013,
  title = {Distribution and abundance of Hector's dolphins off Otago, New Zealand},
  volume = {47},
  issn = {0028-8330},
  url = {http://dx.doi.org/10.1080/00288330.2013.771687},
  doi = {10.1080/00288330.2013.771687},
  abstract = {Data on the distribution and abundance of Hector's dolphins (Cephalorhynchus hectori) along the Otago coastline, between Taieri Mouth and Ōamaru (approximately 130 km alongshore), were collected in 2010–2011. Alongshore surveys were conducted in small boats, travelling approximately 400 m from shore, at speeds of 10 to 15 knots. Photographic identification was used to establish sighting histories of identifiable individuals, and mark rate (36\%, CV=12\%, CI=28–45). Hector's dolphins were found in two areas: between Taiaroa Head and Cornish Head; and in the vicinity of Moeraki. Encounter records suggest there is little exchange between these zones. The Huggins closed capture method was used to estimate the number of marked individuals between Taiaroa Head and Cornish Head (approximately 22 km alongshore), and this value was scaled by mark rate to estimate a total population size of 42 individuals (CV=41\%, CI=19–92). A closed population Bayesian analysis yielded a similar estimate of 37 individuals (CV=42\%, CI=25–75).},
  timestamp = {2015-03-06 19:16:35},
  number = {2},
  journaltitle = {New Zealand Journal of Marine and Freshwater Research},
  shortjournal = {N. Z. J. Mar. Freshw. Res.},
  author = {Turek, J and Slooten, E and Dawson, S and Rayment, W and Turek, D},
  urldate = {2015-03-06},
  date = {2013-06-01},
  pages = {181--191},
  file = {Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/X7G7USHV/login.html:text/html}
}

@article{Jacquier2007,
  title = {{MCMC} maximum likelihood for latent state models},
  volume = {137},
  issn = {0304-4076},
  url = {http://www.sciencedirect.com/science/article/pii/S0304407606000704},
  doi = {10.1016/j.jeconom.2005.11.017},
  abstract = {This paper develops a pure simulation-based approach for computing maximum likelihood estimates in latent state variable models using Markov Chain Monte Carlo methods (MCMC). Our MCMC algorithm simultaneously evaluates and optimizes the likelihood function without resorting to gradient methods. The approach relies on data augmentation, with insights similar to simulated annealing and evolutionary Monte Carlo algorithms. We prove a limit theorem in the degree of data augmentation and use this to provide standard errors and convergence diagnostics. The resulting estimator inherits the sampling asymptotic properties of maximum likelihood. We demonstrate the approach on two latent state models central to financial econometrics: a stochastic volatility and a multivariate jump-diffusion models. We find that convergence to the MLE is fast, requiring only a small degree of augmentation.},
  timestamp = {2014-12-19 21:36:36},
  number = {2},
  journaltitle = {Journal of Econometrics},
  shortjournal = {Journal of Econometrics},
  author = {Jacquier, Eric and Johannes, Michael and Polson, Nicholas},
  urldate = {2014-12-16},
  date = {2007-04},
  pages = {615--640},
  keywords = {Diffusion,Evolutionary Monte-Carlo,Financial econometrics,Jumps,Maximum likelihood,MCMC,Optimization,Simulated annealing,Stochastic volatility},
  file = {ScienceDirect Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/VI9DHEPT/Jacquier et al. - 2007 - MCMC maximum likelihood for latent state models.pdf:application/pdf;ScienceDirect Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/5Q5UKSRK/S0304407606000704.html:text/html}
}

@article{Boettiger2012,
  title = {Quantifying limits to detection of early warning for critical transitions},
  volume = {9},
  rights = {This journal is © 2012 The Royal Society},
  issn = {1742-5689, 1742-5662},
  url = {http://rsif.royalsocietypublishing.org/content/9/75/2527},
  doi = {10.1098/rsif.2012.0125},
  abstract = {Catastrophic regime shifts in complex natural systems may be averted through advanced detection. Recent work has provided a proof-of-principle that many systems approaching a catastrophic transition may be identified through the lens of early warning indicators such as rising variance or increased return times. Despite widespread appreciation of the difficulties and uncertainty involved in such forecasts, proposed methods hardly ever characterize their expected error rates. Without the benefits of replicates, controls or hindsight, applications of these approaches must quantify how reliable different indicators are in avoiding false alarms, and how sensitive they are to missing subtle warning signs. We propose a model-based approach to quantify this trade-off between reliability and sensitivity and allow comparisons between different indicators. We show these error rates can be quite severe for common indicators even under favourable assumptions, and also illustrate how a model-based indicator can improve this performance. We demonstrate how the performance of an early warning indicator varies in different datasets, and suggest that uncertainty quantification become a more central part of early warning predictions.},
  timestamp = {2015-03-08 03:25:41},
  number = {75},
  journaltitle = {Journal of The Royal Society Interface},
  shortjournal = {J. R. Soc. Interface},
  author = {Boettiger, Carl and Hastings, Alan},
  urldate = {2015-03-08},
  date = {2012-10-07},
  pages = {2527--2539},
  langid = {english},
  file = {2527.full.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/ZUKNEBN7/2527.full.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/M3SU3P5M/login.html:text/html}
}

@article{Lunn2009,
  title = {The {BUGS} project: Evolution, critique and future directions},
  volume = {28},
  rights = {Copyright © 2009 John Wiley \& Sons, Ltd.},
  shorttitle = {The {BUGS} project},
  abstract = {BUGS is a software package for Bayesian inference using Gibbs sampling. The software has been instrumental in raising awareness of Bayesian modelling among both academic and commercial communities internationally, and has enjoyed considerable success over its 20-year life span. Despite this, the software has a number of shortcomings and a principal aim of this paper is to provide a balanced critical appraisal, in particular highlighting how various ideas have led to unprecedented flexibility while at the same time producing negative side effects. We also present a historical overview of the BUGS project and some future perspectives. Copyright © 2009 John Wiley \& Sons, Ltd.},
  timestamp = {2015-03-25 23:37:33},
  number = {25},
  journaltitle = {Statistics in Medicine},
  shortjournal = {Statist. Med.},
  author = {Lunn, David and Spiegelhalter, David and Thomas, Andrew and Best, Nicky},
  date = {2009},
  pages = {3049--3067},
  langid = {english},
  keywords = {Bayesian modelling,BUGS,graphical models,OpenBUGS,WinBUGS},
  file = {Lunn, Spiegelhalter, Thomas, and Best, 2009.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/GKIU5AGS/Lunn, Spiegelhalter, Thomas, and Best, 2009.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/4IEWKTV2/abstract.html:text/html}
}

@article{Neal2006,
  title = {Optimal scaling for partially updating {MCMC} algorithms},
  volume = {16},
  issn = {1050-5164, 2168-8737},
  url = {http://projecteuclid.org/euclid.aoap/1151592241},
  doi = {10.1214/105051605000000791},
  abstract = {In this paper we shall consider optimal scaling problems for high-dimensional Metropolis–Hastings algorithms where updates can be chosen to be lower dimensional than the target density itself. We find that the optimal scaling rule for the Metropolis algorithm, which tunes the overall algorithm acceptance rate to be 0.234, holds for the so-called Metropolis-within-Gibbs algorithm as well. Furthermore, the optimal efficiency obtainable is independent of the dimensionality of the update rule. This has important implications for the MCMC practitioner since high-dimensional updates are generally computationally more demanding, so that lower-dimensional updates are therefore to be preferred. Similar results with rather different conclusions are given for so-called Langevin updates. In this case, it is found that high-dimensional updates are frequently most efficient, even taking into account computing costs.},
  timestamp = {2014-12-19 21:36:43},
  number = {2},
  journaltitle = {The Annals of Applied Probability},
  shortjournal = {Ann. Appl. Probab.},
  author = {Neal, Peter and Roberts, Gareth},
  urldate = {2014-12-16},
  date = {2006-05},
  pages = {475--515},
  keywords = {Langevin algorithm,Markov Chain Monte Carlo,Metropolis algorithm,optimal scaling,weak convergence},
  file = {Neal and Roberts, 2006.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/2Z22REV6/Neal and Roberts, 2006.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/BBSEZBV3/1151592241.html:text/html},
  mrnumber = {MR2244423},
  zmnumber = {1127.60021}
}

@book{Rue2005,
  title = {Gaussian Markov Random Fields: Theory and Applications},
  shorttitle = {Gaussian Markov Random Fields},
  abstract = {Gaussian Markov Random Field (GMRF) models are most widely used in spatial statistics - a very active area of research in which few up-to-date reference works are available. This is the first book on the subject that provides a unified framework of GMRFs with particular emphasis on the computational aspects. This book includes extensive case-studies and, online, a c-library for fast and exact simulation. With chapters contributed by leading researchers in the field, this volume is essential reading for statisticians working in spatial theory and its applications, as well as quantitative researchers in a wide range of science fields where spatial data analysis is important.},
  pagetotal = {281},
  timestamp = {2015-03-25 23:32:55},
  publisher = {{CRC} Press},
  author = {Rue, Havard and Held, Leonhard},
  date = {2005-02-18},
  langid = {english},
  keywords = {Computers / Mathematical \& Statistical Software,Mathematics / Probability \& Statistics / General,Science / Earth Sciences / Geology}
}

@article{Aho1989,
  title = {Code Generation Using Tree Matching and Dynamic Programming},
  volume = {11},
  issn = {0164-0925},
  doi = {10.1145/69558.75700},
  abstract = {Compiler-component generators, such as lexical analyzer generators and parser generators, have long been used to facilitate the construction of compilers. A tree-manipulation language called twig has been developed to help construct efficient code generators. Twig transforms a tree-translation scheme into a code generator that combines a fast top-down tree-pattern matching algorithm with dynamic programming. Twig has been used to specify and construct code generators for several experimental compilers targeted for different machines.},
  timestamp = {2015-03-04 22:23:39},
  number = {4},
  journaltitle = {{ACM} Trans. Program. Lang. Syst.},
  shortjournal = {{ACM} Trans Program Lang Syst},
  author = {Aho, Alfred V. and Ganapathi, Mahadevan and Tjiang, Steven W. K.},
  date = {1989-10},
  pages = {491--516}
}

@article{Liu1994a,
  title = {The Collapsed Gibbs Sampler in Bayesian Computations with Applications to a Gene Regulation Problem},
  volume = {89},
  issn = {0162-1459},
  url = {http://amstat.tandfonline.com/doi/abs/10.1080/01621459.1994.10476829},
  doi = {10.1080/01621459.1994.10476829},
  timestamp = {2014-12-19 21:37:00},
  number = {427},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  author = {Liu, Jun S.},
  urldate = {2014-12-17},
  date = {1994-09-01},
  pages = {958--966},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/SJXIT54U/Liu - 1994 - The Collapsed Gibbs Sampler in Bayesian Computatio.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/J9IQBCNP/01621459.1994.html:text/html}
}

@incollection{Gilks2005,
  title = {Markov Chain Monte Carlo},
  rights = {Copyright © 2005 John Wiley \& Sons, Ltd},
  abstract = {Markov chain Monte Carlo (MCMC) is a technique for estimating by simulation the expectation of a statistic in a complex model. Successive random selections form a Markov chain, the stationary distribution of which is the target distribution. It is particularly useful for the evaluation of posterior distributions in complex Bayesian models. In the Metropolis–Hastings algorithm, items are selected from an arbitrary “proposal” distribution and are retained or not according to an acceptance rule. The Gibbs sampler is a special case in which the proposal distributions are conditional distributions of single components of a vector parameter. Various special cases and applications are considered.},
  timestamp = {2015-03-04 22:33:03},
  booktitle = {Encyclopedia of Biostatistics},
  publisher = {John Wiley \& Sons, Ltd},
  author = {Gilks, W. R.},
  date = {2005},
  langid = {english},
  keywords = {Gibbs sampler,hierarchical,hybrid chain,Metropolis–Hastings,missing data,proposal distribution,reversible jump,stationary distribution},
  file = {Gilks 2005.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/S44CJZIG/Gilks 2005.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/UDZ324ZI/abstract.html:text/html}
}

@article{Thomas2000,
  title = {A review of research on project-based learning},
  timestamp = {2015-07-07 19:12:38},
  author = {Thomas, John W},
  date = {2000},
  year = 2000
}

@book{Gelman2006,
  title = {Data Analysis Using Regression and Multilevel/Hierarchical Models},
  isbn = {978-1-139-46093-4},
  abstract = {Data Analysis Using Regression and Multilevel/Hierarchical Models, first published in 2007, is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout.},
  pagetotal = {651},
  timestamp = {2015-01-16 23:12:13},
  publisher = {Cambridge University Press},
  author = {Gelman, Andrew and Hill, Jennifer},
  date = {2006-12-18},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General,Political Science / General,Psychology / Assessment, Testing \& Measurement,Social Science / Research}
}

@online{Patil2012,
  title = {Data Scientist: The Sexiest Job of the 21st Century},
  shorttitle = {Data Scientist},
  abstract = {Meet the people who can coax treasure out of messy, unstructured data.},
  titleaddon = {Harvard Business Review},
  author = {Patil, Thomas H. {DavenportD} J.},
  year = 2012
}

@online{NIMBLEDevelopmentTeam2014,
  title = {{NIMBLE}: An R Package for Programming with {BUGS} models, Version 0.1},
  url = {http://r-nimble.org},
  timestamp = {2014-12-22 21:15:48},
  author = {{NIMBLE} Development Team, .},
  date = {2014}
}

@article{Levine2001,
  title = {Implementations of the Monte Carlo {EM} Algorithm},
  volume = {10},
  issn = {1061-8600},
  url = {http://dx.doi.org/10.1198/106186001317115045},
  doi = {10.1198/106186001317115045},
  abstract = {The Monte Carlo EM (MCEM) algorithm is a modification of the EM algorithm where the expectation in the E-step is computed numerically through Monte Carlo simulations. The most exible and generally applicable approach to obtaining a Monte Carlo sample in each iteration of an MCEM algorithm is through Markov chain Monte Carlo (MCMC) routines such as the Gibbs and Metropolis–Hastings samplers. Although MCMC estimation presents a tractable solution to problems where the E-step is not available in closed form, two issues arise when implementing this MCEM routine: (1) how do we minimize the computational cost in obtaining an MCMC sample? and (2) how do we choose the Monte Carlo sample size? We address the first question through an application of importance sampling whereby samples drawn during previous EM iterations are recycled rather than running an MCMC sampler each MCEM iteration. The second question is addressed through an application of regenerative simulation. We obtain approximate independent and identical samples by subsampling the generated MCMC sample during different renewal periods. Standard central limit theorems may thus be used to gauge Monte Carlo error. In particular, we apply an automated rule for increasing the Monte Carlo sample size when the Monte Carlo error overwhelms the EM estimate at any given iteration. We illustrate our MCEM algorithm through analyses of two datasets fit by generalized linear mixed models. As a part of these applications, we demonstrate the improvement in computational cost and efficiency of our routine over alternative MCEM strategies.},
  timestamp = {2014-12-19 21:36:30},
  number = {3},
  journaltitle = {Journal of Computational and Graphical Statistics},
  shortjournal = {J. Comput. Graph. Stat.},
  author = {Levine, Richard A and Casella, George},
  urldate = {2014-12-16},
  date = {2001-09-01},
  pages = {422--439},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/2REFMZER/Levine and Casella - 2001 - Implementations of the Monte Carlo EM Algorithm.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/QNGSVVQV/106186001317115045.html:text/html}
}

@book{Pedersen1997,
  title = {The First Universities: Studium Generale and the Origins of University Education in Europe},
  isbn = {978-0-521-59431-8},
  shorttitle = {The First Universities},
  abstract = {This is a general study of the development of higher education in Europe from antiquity until the end of the Middle Ages, set against a background of the social and political history of the period. It shows how the slender traditions of ancient learning, kept alive in the monastic and cathedral schools, was enriched by an enormous influx of knowledge from the Islamic world and how in consequence the schools developed into universities. These early institutions are examined from a variety of points of view, as institutions, as places where ideas spread and as points of interaction with local and national authority. Special attention is paid to early intellectual history and to the scientific disciplines and to the everyday life of the students and their teachers. The book is intended as a broad introduction to the subject for students of the history of education, but it will also attract general readers with only a slight knowledge of the subject.},
  pagetotal = {328},
  timestamp = {2015-07-10 18:32:05},
  publisher = {Cambridge University Press},
  author = {Pedersen, Olaf},
  date = {1997},
  langid = {english},
  keywords = {Education / Higher,Education / History,History / Europe / General,History / Medieval},
  year = 1997
}

@report{Shaby2011,
  title = {Exploring an adaptive Metropolis algorithm},
  timestamp = {2014-12-23 20:25:02},
  number = {2011-14},
  institution = {Department of Statistics, Duke University},
  author = {Shaby, B. and Wells, M.},
  date = {2011}
}

@article{Roberts2007,
  title = {Coupling and Ergodicity of Adaptive Markov Chain Monte Carlo Algorithms},
  volume = {44},
  rights = {Copyright © 2007 Applied Probability Trust},
  issn = {0021-9002},
  abstract = {We consider basic ergodicity properties of adaptive Markov chain Monte Carlo algorithms under minimal assumptions, using coupling constructions. We prove convergence in distribution and a weak law of large numbers. We also give counterexamples to demonstrate that the assumptions we make are not redundant.},
  timestamp = {2014-12-19 21:36:17},
  eprinttype = {jstor},
  eprint = {27595854},
  number = {2},
  journaltitle = {Journal of Applied Probability},
  shortjournal = {Journal of Applied Probability},
  author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
  date = {2007-06-01},
  pages = {458--475},
  file = {JSTOR Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/C9IIX89G/Roberts and Rosenthal - 2007 - Coupling and Ergodicity of Adaptive Markov Chain M.pdf:application/pdf}
}

@book{pedersen_first_1997,
  title = {The First Universities: Studium Generale and the Origins of University Education in Europe},
  isbn = {978-0-521-59431-8},
  shorttitle = {The First Universities},
  abstract = {This is a general study of the development of higher education in Europe from antiquity until the end of the Middle Ages, set against a background of the social and political history of the period. It shows how the slender traditions of ancient learning, kept alive in the monastic and cathedral schools, was enriched by an enormous influx of knowledge from the Islamic world and how in consequence the schools developed into universities. These early institutions are examined from a variety of points of view, as institutions, as places where ideas spread and as points of interaction with local and national authority. Special attention is paid to early intellectual history and to the scientific disciplines and to the everyday life of the students and their teachers. The book is intended as a broad introduction to the subject for students of the history of education, but it will also attract general readers with only a slight knowledge of the subject.},
  pagetotal = {328},
  timestamp = {2015-07-10 18:31:22},
  publisher = {Cambridge University Press},
  author = {Pedersen, Olaf},
  date = {1997},
  langid = {english},
  keywords = {Education / Higher,Education / History,History / Europe / General,History / Medieval}
}

@book{Banerjee2003,
  title = {Hierarchical Modeling and Analysis for Spatial Data},
  abstract = {Among the many uses of hierarchical modeling, their application to the statistical analysis of spatial and spatio-temporal data from areas such as epidemiology And environmental science has proven particularly fruitful. Yet to date, the few books that address the subject have been either too narrowly focused on specific aspects of spatial analysis, or written at a level often inaccessible to those lacking a strong background in mathematical statistics.Hierarchical Modeling and Analysis for Spatial Data is the first accessible, self-contained treatment of hierarchical methods, modeling, and data analysis for spatial and spatio-temporal data. Starting with overviews of the types of spatial data, the data analysis tools appropriate for each, and a brief review of the Bayesian approach to statistics, the authors discuss hierarchical modeling for univariate spatial response data, including Bayesian kriging and lattice (areal data) modeling. They then consider the problem of spatially misaligned data, methods for handling multivariate spatial responses, spatio-temporal models, and spatial survival models. The final chapter explores a variety of special topics, including spatially varying coefficient models.This book provides clear explanations, plentiful illustrations --some in full color--a variety of homework problems, and tutorials and worked examples using some of the field's most popular software packages.. Written by a team of leaders in the field, it will undoubtedly remain the primary textbook and reference on the subject for years to come.},
  pagetotal = {470},
  timestamp = {2015-03-04 22:29:17},
  publisher = {{CRC} Press},
  author = {Banerjee, Sudipto and Carlin, Bradley P. and Gelfand, Alan E.},
  date = {2003-12-17},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General,Science / Life Sciences / Biology}
}

@incollection{Hayashi1998,
  title = {What is Data Science? Fundamental Concepts and a Heuristic Example},
  rights = {©1998 Springer-Verlag Tokyo},
  isbn = {978-4-431-70208-5 978-4-431-65950-1},
  url = {http://link.springer.com/chapter/10.1007/978-4-431-65950-1_3},
  shorttitle = {What is Data Science ?},
  abstract = {Data Science is not only a synthetic concept to unify statistics, data analysis and their related methods but also comprises its results. It includes three phases, design for data, collection of data, and analysis on data. Fundamental concepts and various methods based on it are discussed with a heuristic example.},
  timestamp = {2015-07-07 23:13:19},
  booktitle = {Data Science, Classification, and Related Methods},
  series = {Studies in Classification, Data Analysis, and Knowledge Organization},
  publisher = {Springer Japan},
  author = {Hayashi, Chikio},
  editor = {Hayashi, Prof Emeritus Chikio and Yajima, Prof Keiji and Bock, Prof Hans-Hermann and Ohsumi, Prof Noboru and Tanaka, Prof Yutaka and Baba, Prof Yasumasa},
  urldate = {2015-07-07},
  date = {1998},
  pages = {40--51},
  langid = {english},
  keywords = {Data Structures,Economic Theory,Statistics, general},
  file = {Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/9B2BJMFZ/login.html:text/html},
  year = 1998
}

@article{Russell2010,
  title = {Artificial intelligence: a modern approach},
  timestamp = {2015-03-10 18:15:32},
  journaltitle = {Prentice Hall series in artificial intelligence},
  shortjournal = {Prentice Hall Ser. Artif. Intell.},
  author = {Russell, {SJ} and Norvig, P and Davis, E},
  date = {2010}
}

@book{Harvey1993,
  location = {Cambridge, Massachusetts},
  edition = {2},
  title = {Time series models},
  timestamp = {2014-12-19 21:37:02},
  publisher = {The {MIT} Press},
  author = {Harvey, Andrew C},
  date = {1993}
}

@article{Thompson2010,
  title = {A Comparison of Methods for Computing Autocorrelation Time},
  url = {http://arxiv.org/abs/1011.0175},
  abstract = {This paper describes four methods for estimating autocorrelation time and evaluates these methods with a test set of seven series. Fitting an autoregressive process appears to be the most accurate method of the four. An R package is provided for extending the comparison to more methods and test series.},
  timestamp = {2014-12-19 21:36:00},
  author = {Thompson, Madeleine B.},
  urldate = {2014-11-24},
  date = {2010-10-31},
  keywords = {G.3,Statistics - Computation},
  file = {arXiv\:1011.0175 PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/DKESZN6G/Thompson - 2010 - A Comparison of Methods for Computing Autocorrelat.pdf:application/pdf;arXiv.org Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/S5H8TCCI/1011.html:text/html},
  eprinttype = {arxiv},
  eprint = {1011.0175}
}

@article{Jones1996,
  title = {An Introduction to Partial Evaluation},
  volume = {28},
  issn = {0360-0300},
  doi = {10.1145/243439.243447},
  abstract = {Partial evaluation provides a unifying paradigm for a broad spectrum of work in program optimization compiling interpretation and the generation of automatic program generators {[}Bjørner et al. 1987; Ershov 1992; and Jones et al. 1993]. It is a program optimization technique, perhaps better called program specialization, closely related to but different from Jørring and Scherlis' staging transformations {[}1986]. It emphasizes, in comparison with Burstall and Darlington {[}1977] and Jørring and Scherlis {[}1986] and other program transformation work, full automation and the generation of program generators as well as transforming single programs. Much partial evaluation work to date has concerned   automatic compiler generation from an interpretive definition of programming language, but it also has important applications to scientific computing, logic programming, metaprogramming, and expert systems; some pointers are given later.},
  timestamp = {2015-03-04 22:24:23},
  number = {3},
  journaltitle = {{ACM} Comput. Surv.},
  shortjournal = {{ACM} Comput Surv},
  author = {Jones, Neil D.},
  date = {1996-09},
  pages = {480--503},
  keywords = {compiler generators,compilers,interpreters,partial evaluation,program specialization},
  file = {p480-jones.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/GDW8S3FI/p480-jones.pdf:application/pdf}
}

@incollection{Kitagawa1996,
  title = {Linear Gaussian State Space Modeling},
  rights = {©1996 Springer-Verlag New York, Inc.},
  isbn = {978-0-387-94819-5 978-1-4612-0761-0},
  url = {http://link.springer.com/chapter/10.1007/978-1-4612-0761-0_5},
  abstract = {Linear Gaussian state space modeling is treated in this chapter. The prediction, filtering and smoothing formulas in the standard Kalman filter are shown. Model identification or, computation of the likelihood of the model is also treated. Some of the well known state space models that are used in this book as well as state space modeling of missing observations and a state space model for unequally spaced time series are shown. The final section is a discussion of the information square root filter/smoother, that we use in linear Gaussian state space seasonal decomposition modeling in Chapter 9. Not necessarily linear - not necessarily Gaussian state space modeling is treated in Chapter 6. A variety of illustrative examples of linear state space modeling is shown in Chapter 7.},
  timestamp = {2014-12-19 21:36:33},
  number = {116},
  booktitle = {Smoothness Priors Analysis of Time Series},
  series = {Lecture Notes in Statistics},
  publisher = {Springer New York},
  author = {Kitagawa, Genshiro and Gersch, Will},
  urldate = {2014-12-05},
  date = {1996-01-01},
  pages = {55--65},
  langid = {english},
  keywords = {Analysis,Statistics, general},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/2HVCTKP3/Kitagawa and Gersch - 1996 - Linear Gaussian State Space Modeling.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/2DAQTH6V/Kitagawa and Gersch - 1996 - Linear Gaussian State Space Modeling.html:text/html}
}

@article{Herreid2013,
  title = {Case studies and the flipped classroom},
  volume = {42},
  timestamp = {2015-07-07 18:59:40},
  number = {5},
  journaltitle = {Journal of College Science Teaching},
  shortjournal = {J. Coll. Sci. Teach.},
  author = {Herreid, Clyde Freeman and Schiller, Nancy A},
  date = {2013},
  pages = {62--66},
  year = 2013
}

@book{Carlin2000,
  title = {Bayes and empirical Bayes methods for data analysis},
  timestamp = {2014-12-19 21:36:09},
  publisher = {{CRC} Press},
  author = {Carlin, Bradley P and Louis, Thomas A},
  date = {2000}
}

@article{raftery_using_2005,
  title = {Using Bayesian Model Averaging to Calibrate Forecast Ensembles},
  volume = {133},
  issn = {0027-0644},
  url = {http://journals.ametsoc.org/doi/abs/10.1175/MWR2906.1},
  doi = {10.1175/MWR2906.1},
  abstract = {Abstract Ensembles used for probabilistic weather forecasting often exhibit a spread-error correlation, but they tend to be underdispersive. This paper proposes a statistical method for postprocessing ensembles based on Bayesian model averaging (BMA), which is a standard method for combining predictive distributions from different sources. The BMA predictive probability density function (PDF) of any quantity of interest is a weighted average of PDFs centered on the individual bias-corrected forecasts, where the weights are equal to posterior probabilities of the models generating the forecasts and reflect the models' relative contributions to predictive skill over the training period. The BMA weights can be used to assess the usefulness of ensemble members, and this can be used as a basis for selecting ensemble members; this can be useful given the cost of running large ensembles. The BMA PDF can be represented as an unweighted ensemble of any desired size, by simulating from the BMA predictive distribution. The BMA predictive variance can be decomposed into two components, one corresponding to the between-forecast variability, and the second to the within-forecast variability. Predictive PDFs or intervals based solely on the ensemble spread incorporate the first component but not the second. Thus BMA provides a theoretical explanation of the tendency of ensembles to exhibit a spread-error correlation but yet be underdispersive. The method was applied to 48-h forecasts of surface temperature in the Pacific Northwest in January–June 2000 using the University of Washington fifth-generation Pennsylvania State University–NCAR Mesoscale Model (MM5) ensemble. The predictive PDFs were much better calibrated than the raw ensemble, and the BMA forecasts were sharp in that 90\% BMA prediction intervals were 66\% shorter on average than those produced by sample climatology. As a by-product, BMA yields a deterministic point forecast, and this had root-mean-square errors 7\% lower than the best of the ensemble members and 8\% lower than the ensemble mean. Similar results were obtained for forecasts of sea level pressure. Simulation experiments show that BMA performs reasonably well when the underlying ensemble is calibrated, or even overdispersed.},
  timestamp = {2015-07-21 21:08:08},
  number = {5},
  journaltitle = {Monthly Weather Review},
  shortjournal = {Mon. Wea. Rev.},
  author = {Raftery, Adrian E. and Gneiting, Tilmann and Balabdaoui, Fadoua and Polakowski, Michael},
  urldate = {2015-07-21},
  date = {2005-05-01},
  pages = {1155--1174},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/8ZSQ49HN/Raftery et al. - 2005 - Using Bayesian Model Averaging to Calibrate Foreca.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/KIMH3RTH/Raftery et al. - 2005 - Using Bayesian Model Averaging to Calibrate Foreca.html:text/html}
}

@article{Barone1990,
  title = {Improving Stochastic Relaxation for Gussian Random Fields},
  volume = {4},
  abstract = {In this paper, we are concerned with the simulation of Gaussian random fields by means of iterative stochastic algorithms, which are compared in terms of rate of convergence. A parametrized class of algorithms, which includes stochastic relaxation (Gibbs sampler), is proposed and its convergence properties are established. A suitable choice for the parameter improves the rate of convergence with respect to stochastic relaxation for special classes of covariance matrices. Some examples and numerical experiments are given.},
  timestamp = {2015-03-04 22:29:32},
  number = {03},
  journaltitle = {Probability in the Engineering and Informational Sciences},
  shortjournal = {Probab. Eng. Informational Sci.},
  author = {Barone, Piero and Frigessi, Arnolodo},
  date = {1990-07},
  pages = {369--389},
  file = {Cambridge Journals Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/W5BZ2APS/displayAbstract.html:text/html;Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/W86R39RQ/Barone and Frigessi - 1990 - Improving Stochastic Relaxation for Gussian Random.pdf:application/pdf}
}

@article{Gelman2006a,
  title = {Multilevel (Hierarchical) Modeling: What It Can and Cannot Do},
  volume = {48},
  issn = {0040-1706},
  doi = {10.1198/004017005000000661},
  shorttitle = {Multilevel (Hierarchical) Modeling},
  timestamp = {2015-03-04 22:24:01},
  number = {3},
  journaltitle = {Technometrics},
  shortjournal = {Technometrics},
  author = {Gelman, Andrew},
  date = {2006-08-01},
  pages = {432--435},
  file = {Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/X9VJHQI3/login.html:text/html}
}

@article{Marjoram2003,
  title = {Markov chain Monte Carlo without likelihoods},
  volume = {100},
  abstract = {Many stochastic simulation approaches for generating observations from a posterior distribution depend on knowing a likelihood function. However, for many complex probability models, such likelihoods are either impossible or computationally prohibitive to obtain. Here we present a Markov chain Monte Carlo method for generating observations from a posterior distribution without the use of likelihoods. It can also be used in frequentist applications, in particular for maximum-likelihood estimation. The approach is illustrated by an example of ancestral inference in population genetics. A number of open problems are highlighted in the discussion.},
  timestamp = {2015-03-25 23:36:19},
  number = {26},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {{PNAS}},
  author = {Marjoram, Paul and Molitor, John and Plagnol, Vincent and Tavaré, Simon},
  date = {2003-12-23},
  pages = {15324--15328},
  langid = {english},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/RM3BGHQW/Marjoram et al. - 2003 - Markov chain Monte Carlo without likelihoods.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/WW3I927F/Marjoram et al. - 2003 - Markov chain Monte Carlo without likelihoods.html:text/html}
}

@article{Harte2008,
  title = {Maximum entropy and the state-variable approach to macroecology},
  volume = {89},
  issn = {0012-9658},
  url = {http://www.esajournals.org/doi/abs/10.1890/07-1369.1},
  doi = {10.1890/07-1369.1},
  abstract = {The biodiversity scaling metrics widely studied in macroecology include the species–area relationship (SAR), the scale-dependent species–abundance distribution (SAD), the distribution of masses or metabolic energies of individuals within and across species, the abundance–energy or abundance–mass relationship across species, and the species-level occupancy distributions across space. We propose a theoretical framework for predicting the scaling forms of these and other metrics based on the state-variable concept and an analytical method derived from information theory. In statistical physics, a method of inference based on information entropy results in a complete macro-scale description of classical thermodynamic systems in terms of the state variables volume, temperature, and number of molecules. In analogy, we take the state variables of an ecosystem to be its total area, the total number of species within any specified taxonomic group in that area, the total number of individuals across those species, and the summed metabolic energy rate for all those individuals. In terms solely of ratios of those state variables, and without invoking any specific ecological mechanisms, we show that realistic functional forms for the macroecological metrics listed above are inferred based on information entropy. The Fisher log series SAD emerges naturally from the theory. The SAR is predicted to have negative curvature on a log–log plot, but as the ratio of the number of species to the number of individuals decreases, the SAR becomes better and better approximated by a power law, with the predicted slope z in the range of 0.14–0.20. Using the 3/4 power mass–metabolism scaling relation to relate energy requirements and measured body sizes, the Damuth scaling rule relating mass and abundance is also predicted by the theory. We argue that the predicted forms of the macroecological metrics are in reasonable agreement with the patterns observed from plant census data across habitats and spatial scales. While this is encouraging, given the absence of adjustable fitting parameters in the theory, we further argue that even small discrepancies between data and predictions can help identify ecological mechanisms that influence macroecological patterns.},
  timestamp = {2015-01-28 23:27:45},
  number = {10},
  journaltitle = {Ecology},
  shortjournal = {Ecology},
  author = {Harte, J. and Zillio, T. and Conlisk, E. and Smith, A. B.},
  urldate = {2015-01-24},
  date = {2008-10-01},
  pages = {2700--2711},
  keywords = {biodiversity,endemics–area relationship,energy distribution,macroecology,maximum entropy,metabolic theory,spatial pattern,spatial scaling,species–abundance distribution,species–area relationship,state variables},
  file = {Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/PJ8HA8BF/Harte et al. - 2008 - Maximum entropy and the state-variable approach to.html:text/html}
}

@incollection{Taha2004,
  title = {A Gentle Introduction to Multi-stage Programming},
  rights = {©2004 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-22119-7 978-3-540-25935-0},
  abstract = {Multi-stage programming (MSP) is a paradigm for developing generic software that does not pay a runtime penalty for this generality. This is achieved through concise, carefully-designed language extensions that support runtime code generation and program execution. Additionally, type systems for MSP languages are designed to statically ensure that dynamically generated programs are type-safe, and therefore require no type checking after they are generated. This hands-on tutorial is aimed at the reader interested in learning the basics of MSP practice. The tutorial uses a freely available MSP extension of OCaml called MetaOCaml, and presents a detailed analysis of the issues that arise in staging an interpreter for a small programming language. The tutorial concludes with pointers to various resources that can be used to probe further into related topics.},
  timestamp = {2015-03-16 19:14:22},
  number = {3016},
  booktitle = {Domain-Specific Program Generation},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer Berlin Heidelberg},
  author = {Taha, Walid},
  date = {2004},
  pages = {30--50},
  langid = {english},
  keywords = {Programming Languages, Compilers, Interpreters,Programming Techniques,Software Engineering},
  file = {Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/I2JKPS2V/login.html:text/html}
}

@book{Durbin2012,
  title = {Time Series Analysis by State Space Methods: Second Edition},
  shorttitle = {Time Series Analysis by State Space Methods},
  abstract = {This new edition updates Durbin \& Koopman's important text on the state space approach to time series analysis. The distinguishing feature of state space time series models is that observations are regarded as made up of distinct components such as trend, seasonal, regression elements and disturbance terms, each of which is modelled separately. The techniques that emerge from this approach are very flexible and are capable of handling a much wider range of problems than the main analytical system currently in use for time series analysis, the Box-Jenkins ARIMA system. Additions to this second edition include the filtering of nonlinear and non-Gaussian series. Part I of the book obtains the mean and variance of the state, of a variable intended to measure the effect of an interaction and of regression coefficients, in terms of the observations. Part II extends the treatment to nonlinear and non-normal models. For these, analytical solutions are not available so methods are based on simulation.},
  pagetotal = {370},
  timestamp = {2015-03-04 22:30:51},
  publisher = {Oxford University Press},
  author = {Durbin, James and Koopman, Siem Jan},
  date = {2012-05-03},
  langid = {english},
  keywords = {Business \& Economics / Econometrics,Business \& Economics / Statistics,Mathematics / Applied,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Time Series}
}

@book{Dybvig2003,
  title = {The Scheme Programming Language},
  isbn = {978-0-262-54148-0},
  abstract = {This thoroughly updated edition of The Scheme Programming Language provides anintroduction to Scheme and a definitive reference for standard Scheme, presented in a clear andconcise manner. Written for professionals and students with some prior programming experience, itbegins by leading the programmer gently through the basics of Scheme and continues with anintroduction to some of the more advanced features of the language. Many exercises are presented tohelp reinforce the lessons learned, and answers to the exercises are given in a new appendix. Mostof the remaining chapters are dedicated to the reference material, which describes in detail thestandard features of Scheme included in the Revised Report on Scheme and the ANSI/IEEE standard forScheme.Numerous examples are presented throughout the introductory and reference portions of thetext, and a unique set of extended example programs and applications, with additional exercises, arepresented in the final chapter. Reinforcing the book's utility as a reference text are appendixesthat present the formal syntax of Scheme, a summary of standard forms and procedures, and abibliography of Scheme resources. The Scheme Programming Language stands alone as an introduction toand essential reference for Scheme programmers. It is also useful as a supplementary text for anycourse that uses Scheme.The Scheme Programming Language is illustrated by artist Jean-Pierre Hebert,who writes Scheme programs to extend his ability to create sophisticated works of digitalart.},
  pagetotal = {314},
  timestamp = {2015-03-05 07:56:10},
  publisher = {{MIT} Press},
  author = {Dybvig, R. Kent},
  date = {2003-01},
  langid = {english},
  keywords = {Computers / Programming Languages / General}
}

@book{Trefethen1997,
  title = {Numerical Linear Algebra},
  isbn = {978-0-89871-361-9},
  abstract = {This is a concise, insightful introduction to the field of numerical linear algebra. The clarity and eloquence of the presentation make it popular with teachers and students alike. The text aims to expand the reader's view of the field and to present standard material in a novel way. All of the most important topics in the field are covered with a fresh perspective, including iterative methods for systems of equations and eigenvalue problems and the underlying principles of conditioning and stability. Presentation is in the form of 40 lectures, which each focus on one or two central ideas. The unity between topics is emphasized throughout, with no risk of getting lost in details and technicalities. The book breaks with tradition by beginning with the QR factorization - an important and fresh idea for students, and the thread that connects most of the algorithms of numerical linear algebra.},
  pagetotal = {356},
  timestamp = {2014-12-19 21:36:41},
  publisher = {{SIAM}},
  author = {Trefethen, Lloyd N. and Bau, David},
  date = {1997-06-01},
  langid = {english},
  keywords = {Mathematics / Algebra / General,Mathematics / Algebra / Linear,Mathematics / Applied,Mathematics / Mathematical Analysis,Technology \& Engineering / Engineering (General)}
}

@article{Crainiceanu2005,
  title = {Bayesian Analysis for Penalized Spline Regression Using {WinBUGS}},
  url = {http://ro.uow.edu.au/eispapers/2517},
  timestamp = {2015-03-06 19:14:39},
  journaltitle = {Faculty of Engineering and Information Sciences - Papers},
  shortjournal = {Fac. Eng. Inf. Sci. - Pap.},
  author = {Crainiceanu, Ciprian and Ruppert, David and Wand, M.},
  date = {2005-01-01},
  pages = {1--24},
  file = {"Bayesian Analysis for Penalized Spline Regression Using WinBUGS" by Ciprian Crainiceanu, David Ruppert et al.:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/JXZC7KD5/2517.html:text/html}
}

@online{_david,
  title = {David A. Kolb on experiential learning.},
  url = {http://infed.org/mobi/david-a-kolb-on-experiential-learning/},
  abstract = {David A. Kolb on experiential learning. David A. Kolb's model of experiential learning can be found in many discussions of the theory and practice of adult education, informal education and lifelon...},
  timestamp = {2015-08-05 02:56:17},
  titleaddon = {infed.org},
  author = {Kolb, David A.},
  urldate = {2015-08-05},
  date = {2010},
  file = {Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/2PHI9CE3/david-a-kolb-on-experiential-learning.html:text/html}
}

@article{Liu1994,
  title = {Covariance structure of the Gibbs sampler with applications to the comparisons of estimators and augmentation schemes},
  volume = {81},
  issn = {0006-3444, 1464-3510},
  url = {http://biomet.oxfordjournals.org/content/81/1/27},
  doi = {10.1093/biomet/81.1.27},
  abstract = {We study the covariance structure of a Markov chain generated by the Gibbs sampler, with emphasis on data augmentation. When applied to a Bayesian missing data problem, the Gibbs sampler produces two natural approximations for the posterior distribution of the parameter vector: the empirical distribution based on the sampled values of the parameter vector, and a mixture of complete data posteriors. We prove that Rao-Blackwellization causes a one-lag delay for the autocovariances among dependent samples obtained from data augmentation, and consequently, the mixture approximation produces estimates with smaller variances than the empirical approximation. The covariance structure results are used to compare different augmentation schemes. It is shown that collapsing and grouping random components in a Gibbs sampler with two or three components usually result in more efficient sampling schemes.},
  timestamp = {2014-12-19 21:36:18},
  number = {1},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  author = {Liu, Jun S. and Wong, Wing Hung and Kong, Augustine},
  urldate = {2014-12-17},
  date = {1994-03-01},
  pages = {27--40},
  langid = {english},
  keywords = {Data augmentation,Empirical and mixture estimators,Forward operator,Interleaving Markov property,Maximal correlation,Rao-Blackwellization},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/NXVI7T43/Liu et al. - 1994 - Covariance structure of the Gibbs sampler with app.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/KZTEV4EG/27.html:text/html}
}

@article{Roberts2001,
  title = {Optimal scaling for various Metropolis-Hastings algorithms},
  volume = {16},
  issn = {0883-4237, 2168-8745},
  url = {http://projecteuclid.org/euclid.ss/1015346320},
  doi = {10.1214/ss/1015346320},
  abstract = {We review and extend results related to optimal scaling of Metropolis–Hastings algorithms. We present various theoretical results for the high-dimensional limit. We also present simulation studies which confirm the theoretical results in finite-dimensional contexts.},
  timestamp = {2014-12-19 21:36:44},
  number = {4},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  author = {Roberts, Gareth O. and Rosenthal, Jeffrey S.},
  urldate = {2014-11-19},
  date = {2001-11},
  pages = {351--367},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/SPSRXBBG/Roberts and Rosenthal - 2001 - Optimal scaling for various Metropolis-Hastings al.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/A3U9KJKI/Roberts and Rosenthal - 2001 - Optimal scaling for various Metropolis-Hastings al.html:text/html},
  mrnumber = {MR1888450},
  zmnumber = {02068937}
}

@inproceedings{Taha1997,
  location = {New York, {NY}, {USA}},
  title = {Multi-stage Programming with Explicit Annotations},
  isbn = {0-89791-917-3},
  doi = {10.1145/258993.259019},
  abstract = {We introduce MetaML, a statically-typed multi-stage programming language extending Nielson and Nielson's two stage notation to an arbitrary number of stages. MetaML extends previous work by introducing four distinct staging annotations which generalize those published previously {[}25, 12, 7, 6]We give a static semantics in which type checking is done once and for all before the first stage, and a dynamic semantics which introduces a new concept of cross-stage persistence, which requires that variables available in any stage are also available in all future stages.We illustrate that staging is a manual form of binding time analysis. We explain why, even in the presence of automatic binding time analysis, explicit annotations are useful, especially for programs with more than two stages.A thesis of this paper is that multi-stage languages are useful as programming languages in their own right, and should support features that make it possible for programmers to write staged computations without significantly changing their normal programming style. To illustrate this we provide a simple three stage example, and an extended two-stage example elaborating a number of practical issues.},
  timestamp = {2015-03-04 22:24:46},
  booktitle = {Proceedings of the 1997 {ACM} {SIGPLAN} Symposium on Partial Evaluation and Semantics-based Program Manipulation},
  series = {{PEPM} '97},
  publisher = {{ACM}},
  author = {Taha, Walid and Sheard, Tim},
  date = {1997},
  pages = {203--217},
  file = {p203-taha.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/8N44PX5R/p203-taha.pdf:application/pdf}
}

@book{Efron1994,
  title = {An Introduction to the Bootstrap},
  isbn = {978-0-412-04231-7},
  abstract = {Statistics is a subject of many uses and surprisingly few effective practitioners. The traditional road to statistical knowledge is blocked, for most, by a formidable wall of mathematics. The approach in An Introduction to the Bootstrap avoids that wall. It arms scientists and engineers, as well as statisticians, with the computational techniques they need to analyze and understand complicated data sets.},
  pagetotal = {456},
  timestamp = {2014-12-19 21:36:07},
  publisher = {{CRC} Press},
  author = {Efron, Bradley and Tibshirani, R. J.},
  date = {1994-05-15},
  langid = {english},
  keywords = {Computers / Mathematical \& Statistical Software,Mathematics / General,Mathematics / Probability \& Statistics / General}
}

@article{Breslow1993,
  title = {Approximate Inference in Generalized Linear Mixed Models},
  volume = {88},
  timestamp = {2015-03-04 22:30:02},
  number = {421},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  author = {Breslow, N. E. and Clayton, D. G.},
  date = {1993-03-01},
  pages = {9--25},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/3QUPNXJH/Breslow and Clayton - 1993 - Approximate Inference in Generalized Linear Mixed .pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/WEGE7VMH/01621459.1993.html:text/html}
}

@book{Entwistle1992,
  title = {The impact of teaching on learning outcomes in higher education: a literature review},
  timestamp = {2015-07-07 18:50:58},
  publisher = {Committee of Vice-Chancellors and Principals of the Universities of the United Kingdom, Universities' Staff Development Unit},
  author = {Entwistle, Noel James},
  date = {1992},
  year = 1992
}

@article{Wei1990,
  title = {A Monte Carlo Implementation of the {EM} Algorithm and the Poor Man's Data Augmentation Algorithms},
  volume = {85},
  issn = {0162-1459},
  url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1990.10474930},
  doi = {10.1080/01621459.1990.10474930},
  abstract = {The first part of this article presents the Monte Carlo implementation of the E step of the EM algorithm. Given the current guess to the maximizer of the posterior distribution, latent data patterns are generated from the conditional predictive distribution. The expected value of the augmented log-posterior is then updated as a mixture of augmented log-posteriors, mixed over the generated latent data patterns (multiple imputations). In the M step of the algorithm, this mixture is maximized to obtain the update to the maximizer of the observed posterior. The gradient and Hessian of the observed log posterior are also expressed as mixtures, mixed over the multiple imputations. The relation between the Monte Carlo EM (MCEM) algorithm and the data augmentation algorithm is noted. Two modifications to the MCEM algorithm (the poor man's data augmentation algorithms), which allow for the calculation of the entire posterior, are then presented. These approximations serve as diagnostics for the validity of the normal approximation to the posterior, as well as starting points for the full data augmentation analysis. The methodology is illustrated with two examples.},
  timestamp = {2014-12-19 21:36:04},
  number = {411},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {J. Am. Stat. Assoc.},
  author = {Wei, Greg C. G. and Tanner, Martin A.},
  urldate = {2014-12-16},
  date = {1990-09-01},
  pages = {699--704},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/QI7ADSQR/Wei and Tanner - 1990 - A Monte Carlo Implementation of the EM Algorithm a.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/G972PWQX/01621459.1990.html:text/html}
}

@article{Cleveland2001,
  title = {Data Science: an Action Plan for Expanding the Technical Areas of the Field of Statistics},
  volume = {69},
  issn = {1751-5823},
  url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1751-5823.2001.tb00477.x/abstract},
  doi = {10.1111/j.1751-5823.2001.tb00477.x},
  shorttitle = {Data Science},
  abstract = {An action plan to enlarge the technical areas of statistics focuses on the data analyst. The plan sets out six technical areas of work for a university department, and advocates a specific allocation of resources devoted to research in each area and to courses in each area. The value of technical work is judged by the extent to which it benefits the data analyst, either directly or indirectly. The plan is also applicable to government research labs and corporate research organizations.},
  timestamp = {2015-07-07 23:04:46},
  number = {1},
  journaltitle = {International Statistical Review},
  shortjournal = {Int. Stat. Rev.},
  author = {Cleveland, William S.},
  urldate = {2015-07-07},
  date = {2001-04-01},
  pages = {21--26},
  langid = {english},
  keywords = {Applications,Computing,Future,Methods,MODELS,Theory},
  file = {Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/DPQXRW57/login.html:text/html}
}

@article{Neal2003,
  title = {Slice Sampling},
  volume = {31},
  rights = {Copyright © 2003 Institute of Mathematical Statistics},
  issn = {0090-5364},
  abstract = {Markov chain sampling methods that adapt to characteristics of the distribution being sampled can be constructed using the principle that one can sample from a distribution by sampling uniformly from the region under the plot of its density function. A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal "slice" defined by the current vertical position, or more generally, with some update that leaves the uniform distribution over this slice invariant. Such "slice sampling" methods are easily implemented for univariate distributions, and can be used to sample from a multivariate distribution by updating each variable in turn. This approach is often easier to implement than Gibbs sampling and more efficient than simple Metropolis updates, due to the ability of slice sampling to adaptively choose the magnitude of changes made. It is therefore attractive for routine and automated use. Slice sampling methods that update all variables simultaneously are also possible. These methods can adaptively choose the magnitudes of changes made to each variable, based on the local properties of the density function. More ambitiously, such methods could potentially adapt to the dependencies between variables by constructing local quadratic approximations. Another approach is to improve sampling efficiency by suppressing random walks. This can be done for univariate slice sampling by "overrelaxation," and for multivariate slice sampling by "reflection" from the edges of the slice.},
  timestamp = {2014-12-19 21:36:52},
  eprinttype = {jstor},
  eprint = {3448413},
  number = {3},
  journaltitle = {The Annals of Statistics},
  shortjournal = {The Annals of Statistics},
  author = {Neal, Radford M.},
  date = {2003-06-01},
  pages = {705--741},
  file = {JSTOR Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/9ZTK8AVE/Neal - 2003 - Slice Sampling.pdf:application/pdf}
}

@article{Straatsma1986,
  title = {Estimation of statistical errors in molecular simulation calculations},
  volume = {57},
  issn = {0026-8976},
  url = {http://dx.doi.org/10.1080/00268978600100071},
  doi = {10.1080/00268978600100071},
  abstract = {The naive estimation of errors in averages obtained from molecular simulation calculations may lead to serious underestimates due to the strong correlations that usually exist within the series. A commonly employed procedure to overcome this difficulty is to estimate the error from the deviation in the averages over subseries that are considered uncorrelated. However, without knowledge of the correlation the choice of the number of subseries cannot be soundly based. A method to estimate the statistical error using the correlation explicitly without serious additional computational effort and without the need for a tedious examination of the correlation behaviour itself is presented. Results of a test of this method for a molecular dynamics study on the polarization energy of xenon in liquid water are given and a comparison is made with the procedure based on subaverages and with a recently suggested method by Smith and Wells {[}1].},
  timestamp = {2014-12-19 21:36:22},
  number = {1},
  journaltitle = {Molecular Physics},
  shortjournal = {Mol. Phys.},
  author = {Straatsma, T.P. and Berendsen, H.J.C. and Stam, A.J.},
  urldate = {2014-11-25},
  date = {1986-01-01},
  pages = {89--95},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/IW2NPIWJ/Straatsma et al. - 1986 - Estimation of statistical errors in molecular simu.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/G5IHCTRR/00268978600100071.html:text/html}
}

@article{Marshall2012,
  title = {An adaptive approach to Langevin {MCMC}},
  volume = {22},
  issn = {0960-3174, 1573-1375},
  url = {http://link.springer.com/article/10.1007/s11222-011-9276-6},
  doi = {10.1007/s11222-011-9276-6},
  abstract = {We consider a class of adaptive MCMC algorithms using a Langevin-type proposal density. We state and prove regularity conditions for the convergence of these algorithms. In addition to these theoretical results we introduce a number of methodological innovations that can be applied much more generally. We assess the performance of these algorithms with simulation studies, including an example of the statistical analysis of a point process driven by a latent log-Gaussian Cox process.},
  timestamp = {2014-12-19 21:36:05},
  number = {5},
  journaltitle = {Statistics and Computing},
  shortjournal = {Stat Comput},
  author = {Marshall, Tristan and Roberts, Gareth},
  urldate = {2014-12-17},
  date = {2012-09-01},
  pages = {1041--1057},
  langid = {english},
  keywords = {Adaptive Markov Chain Monte Carlo,Artificial Intelligence (incl. Robotics),Langevin algorithm,log-Gaussian Cox process,Mathematics, general,Metropolis-Hastings,Numeric Computing,Statistics and Computing/Statistics Programs,Statistics, general},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/Z6E6KN3I/Marshall and Roberts - 2012 - An adaptive approach to Langevin MCMC.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/6QEUN6QR/s11222-011-9276-6.html:text/html}
}

@book{repko2008interdisciplinary,
  title = {Interdisciplinary research: Process and theory},
  timestamp = {2015-08-05 18:00:46},
  publisher = {Sage},
  author = {Repko, Allen F},
  date = {2008},
  year = 2008
}

@article{Gneiting2007,
  title = {Strictly Proper Scoring Rules, Prediction, and Estimation},
  volume = {102},
  issn = {0162-1459},
  url = {http://dx.doi.org/10.1198/016214506000001437},
  doi = {10.1198/016214506000001437},
  abstract = {Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. A scoring rule is proper if the forecaster maximizes the expected score for an observation drawn from the distributionF if he or she issues the probabilistic forecast F, rather than G ≠ F. It is strictly proper if the maximum is unique. In prediction problems, proper scoring rules encourage the forecaster to make careful assessments and to be honest. In estimation problems, strictly proper scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand. This article reviews and develops the theory of proper scoring rules on general probability spaces, and proposes and discusses examples thereof. Proper scoring rules derive from convex functions and relate to information measures, entropy functions, and Bregman divergences. In the case of categorical variables, we prove a rigorous version of the Savage representation. Examples of scoring rules for probabilistic forecasts in the form of predictive densities include the logarithmic, spherical, pseudospherical, and quadratic scores. The continuous ranked probability score applies to probabilistic forecasts that take the form of predictive cumulative distribution functions. It generalizes the absolute error and forms a special case of a new and very general type of score, the energy score. Like many other scoring rules, the energy score admits a kernel representation in terms of negative definite functions, with links to inequalities of Hoeffding type, in both univariate and multivariate settings. Proper scoring rules for quantile and interval forecasts are also discussed. We relate proper scoring rules to Bayes factors and to cross-validation, and propose a novel form of cross-validation known as random-fold cross-validation. A case study on probabilistic weather forecasts in the North American Pacific Northwest illustrates the importance of propriety. We note optimum score approaches to point and quantile estimation, and propose the intuitively appealing interval score as a utility function in interval estimation that addresses width as well as coverage.},
  timestamp = {2014-12-19 21:36:55},
  number = {477},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {J. Am. Stat. Assoc.},
  author = {Gneiting, Tilmann and Raftery, Adrian E},
  urldate = {2014-12-16},
  date = {2007-03-01},
  pages = {359--378},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/WFMQF7A6/Gneiting and Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Est.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/EMDB3VQ9/016214506000001437.html:text/html}
}

@article{Haario2001,
  title = {An Adaptive Metropolis Algorithm},
  volume = {7},
  rights = {Copyright © 2001 International Statistical Institute ({ISI}) and Bernoulli Society for Mathematical Statistics and Probability},
  issn = {1350-7265},
  doi = {10.2307/3318737},
  abstract = {A proper choice of a proposal distribution for Markov chain Monte Carlo methods, for example for the Metropolis-Hastings algorithm, is well known to be a crucial factor for the convergence of the algorithm. In this paper we introduce an adaptive Metropolis (AM) algorithm, where the Gaussian proposal distribution is updated along the process using the full information cumulated so far. Due to the adaptive nature of the process, the AM algorithm is non-Markovian, but we establish here that it has the correct ergodic properties. We also include the results of our numerical tests, which indicate that the AM algorithm competes well with traditional Metropolis-Hastings algorithms, and demonstrate that the AM algorithm is easy to use in practical computation.},
  timestamp = {2014-12-19 21:36:06},
  eprinttype = {jstor},
  eprint = {3318737},
  number = {2},
  journaltitle = {Bernoulli},
  shortjournal = {Bernoulli},
  author = {Haario, Heikki and Saksman, Eero and Tamminen, Johanna},
  date = {2001-04-01},
  pages = {223--242},
  file = {JSTOR Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/UB8STJKR/Haario et al. - 2001 - An Adaptive Metropolis Algorithm.pdf:application/pdf}
}

@article{Taha2000,
  title = {{MetaML} and multi-stage programming with explicit annotations},
  volume = {248},
  issn = {0304-3975},
  doi = {10.1016/S0304-3975(00)00053-0},
  abstract = {We introduce MetaML, a practically motivated, statically typed multi-stage programming language. MetaML is a “real” language. We have built an implementation and used it to solve multi-stage problems. MetaML allows the programmer to construct, combine, and execute code fragments in a type-safe manner. Code fragments can contain free variables, but they obey the static-scoping principle. MetaML performs type-checking for all stages once and for all before the execution of the first stage. Certain anomalies with our first MetaML implementation led us to formalize an illustrative subset of the MetaML implementation. We present both a big-step semantics and type system for this subset, and prove the type system's soundness with respect to a big-step semantics. From a software engineering point of view, this means that generators written in the MetaML subset never generate unsafe programs. A type system and semantics for full MetaML is still ongoing work. We argue that multi-stage languages are useful as programming languages in their own right, that they supply a sound basis for high-level program generation technology, and that they should support features that make it possible for programmers to write staged computations without significantly changing their normal programming style. To illustrate this we provide a simple three-stage example elaborating a number of practical issues. The design of MetaML was based on two main principles that we identified as fundamental for high-level program generation, namely, cross-stage persistence and cross-stage safety. We present these principles, explain the technical problems they give rise to, and how we address with these problems in our implementation.},
  timestamp = {2015-03-04 22:24:52},
  number = {1–2},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  series = {{PEPM}'97},
  author = {Taha, Walid and Sheard, Tim},
  date = {2000-10-06},
  pages = {211--242},
  keywords = {Functional programming,High-level program generation,Multi-level languages,Multi-stage languages,Programming language semantics,Type-safety,Type-systems,λ-Calculus},
  file = {1-s2.0-S0304397500000530-main.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/8V2XG86A/1-s2.0-S0304397500000530-main.pdf:application/pdf;ScienceDirect Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/ZBVEDR76/login.html:text/html}
}

@article{Haario1999,
  title = {Adaptive proposal distribution for random walk Metropolis algorithm},
  volume = {3},
  url = {http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.springer-7c4e533d-3c06-3295-82ad-af648699c3b7},
  timestamp = {2014-12-19 21:36:01},
  journaltitle = {Computational Statistics},
  shortjournal = {Comput. Stat.},
  author = {Haario, Heikki and Saksman, Eero and Tamminen, Johanna},
  urldate = {2014-11-25},
  date = {1999},
  file = {Haario et al.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/3NAA6PW5/Haario et al.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/BEFCRDIC/bwmeta1.element.html:text/html}
}

@article{Hjort2006,
  title = {Post-Processing Posterior Predictive p Values},
  volume = {101},
  issn = {0162-1459},
  url = {http://amstat.tandfonline.com/doi/abs/10.1198/016214505000001393},
  doi = {10.1198/016214505000001393},
  timestamp = {2014-12-19 21:36:49},
  number = {475},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  author = {Hjort, Nils Lid and Dahl, Fredrik A and Steinbakk, Gunnhildur Högnadóttir},
  urldate = {2014-12-16},
  date = {2006-09-01},
  pages = {1157--1174},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/KHQKU2S3/Hjort et al. - 2006 - Post-Processing Posterior Predictive p Values.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/CSA26ZK4/016214505000001393.html:text/html}
}

@article{Fournier2012,
  title = {{AD} Model Builder: using automatic differentiation for statistical inference of highly parameterized complex nonlinear models},
  volume = {27},
  issn = {1055-6788},
  url = {http://dx.doi.org/10.1080/10556788.2011.597854},
  doi = {10.1080/10556788.2011.597854},
  shorttitle = {{AD} Model Builder},
  abstract = {Many criteria for statistical parameter estimation, such as maximum likelihood, are formulated as a nonlinear optimization problem. Automatic Differentiation Model Builder (ADMB) is a programming framework based on automatic differentiation, aimed at highly nonlinear models with a large number of parameters. The benefits of using AD are computational efficiency and high numerical accuracy, both crucial in many practical problems. We describe the basic components and the underlying philosophy of ADMB, with an emphasis on functionality found in no other statistical software. One example of such a feature is the generic implementation of Laplace approximation of high-dimensional integrals for use in latent variable models. We also review the literature in which ADMB has been used, and discuss future development of ADMB as an open source project. Overall, the main advantages of ADMB are flexibility, speed, precision, stability and built-in methods to quantify uncertainty.},
  timestamp = {2014-12-19 21:36:03},
  number = {2},
  journaltitle = {Optimization Methods and Software},
  shortjournal = {Optim. Methods Softw.},
  author = {Fournier, David A. and Skaug, Hans J. and Ancheta, Johnoel and Ianelli, James and Magnusson, Arni and Maunder, Mark N. and Nielsen, Anders and Sibert, John},
  urldate = {2014-12-16},
  date = {2012-04-01},
  pages = {233--249},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/XX6RX937/Fournier et al. - 2012 - AD Model Builder using automatic differentiation .pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/J7TWSZFU/10556788.2011.html:text/html}
}

@article{Roberts1994,
  title = {Integrating Technology into a Teacher Education Program},
  volume = {2},
  issn = {1059-7069},
  abstract = {Describes attempts to integrate computer technology into the preservice teacher education program at Lesley College. Topics discussed include technology goals; faculty attitudes; technology in the schools; technological developments; problems; and factors affecting adoption of computer technology, including the need for faculty to learn more about hardware and software. (six references) (LRW)},
  timestamp = {2015-07-10 18:49:53},
  number = {3},
  journaltitle = {Journal of Technology and Teacher Education},
  shortjournal = {J. Technol. Teach. Educ.},
  author = {Roberts, Nancy and Ferris, Angeline},
  date = {1994-01},
  pages = {215--25},
  langid = {english},
  keywords = {Adoption (Ideas),Computer Assisted Instruction,Computer Literacy,Courseware,Curriculum Development,Educational Objectives,Educational Technology,Elementary Secondary Education,Higher Education,Inservice Teacher Education,Preservice Teacher Education,Problems,Teacher Attitudes,Technological Advancement},
  file = {Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/ADB8ZM8R/eric.ed.gov.html:text/html},
  year = 1994
}

@inproceedings{gordon_novel_1993,
  title = {Novel approach to nonlinear/non-Gaussian Bayesian state estimation},
  volume = {140},
  timestamp = {2015-07-21 18:24:54},
  booktitle = {{IEE} Proceedings F (Radar and Signal Processing)},
  publisher = {{IET}},
  author = {Gordon, Neil J and Salmond, David J and Smith, Adrian {FM}},
  date = {1993},
  pages = {107--113}
}

@inproceedings{Jorring1986,
  location = {New York, {NY}, {USA}},
  title = {Compilers and Staging Transformations},
  doi = {10.1145/512644.512652},
  abstract = {Computations can generally be separated into stages, which are distinguished from one another by either frequency of execution or availability of data. \emph{Precomputation} and \emph{frequency reduction} involve moving computation among a collection of stages so that work is done as early as possible (so less time is required in later steps) and as infrequently as possible (to reduce overall time).We present, by means of examples, several general transformation techniques for carrying out precomputation transformations. We illustrate the techniques by deriving fragments of simple compilers from interpreters, including an example of Prolog compilation, but the techniques are applicable in a broad range of circumstances. Our aim is to demonstrate how perspicuous accounts of precomputation and frequency reduction can be given for a wide range of applications using a small number of relatively straightforward techniques.Related work in partial evaluation, semantically directed compilation, and compiler optimization is discussed.},
  timestamp = {2015-03-04 22:24:28},
  booktitle = {Proceedings of the 13th {ACM} {SIGACT}-{SIGPLAN} Symposium on Principles of Programming Languages},
  series = {{POPL} '86},
  publisher = {{ACM}},
  author = {Jørring, Ulrik and Scherlis, William L.},
  date = {1986},
  pages = {86--96},
  file = {p86-jorring.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/ZSTV9FS9/p86-jorring.pdf:application/pdf}
}

@book{Loukides2011,
  title = {What is data science?},
  timestamp = {2015-07-07 23:11:43},
  publisher = {O'Reilly Media, Inc.},
  author = {Loukides, Mike},
  date = {2011},
  year = 2011
}

@book{Everitt2011,
  location = {Hoboken},
  edition = {5th ed},
  title = {Cluster Analysis},
  isbn = {978-0-470-97781-1},
  url = {http://uclibs.org/PID/176035},
  timestamp = {2014-12-19 21:36:14},
  series = {Wiley series in probability and statistics},
  publisher = {Wiley},
  author = {Everitt, Brian},
  urldate = {2014-11-19},
  date = {2011},
  keywords = {Cluster analysis,clustering,MATHEMATICS / Probability \& Statistics / Multivariate Analysis}
}

@article{Stuart2004,
  title = {Conditional Path Sampling of {SDEs} and the Langevin {MCMC} Method},
  volume = {2},
  issn = {1539-6746, 1945-0796},
  url = {http://projecteuclid.org/euclid.cms/1109885503},
  abstract = {We introduce a stochastic PDE based approach to sampling paths of SDEs, conditional on observations. The SPDEs are derived by generalising the Langevin MCMC method to infinite dimensions. Various applications are described, including sampling paths subject to two end-point conditions (bridges) and nonlinear filter/smoothers.},
  timestamp = {2014-12-19 21:36:16},
  number = {4},
  journaltitle = {Communications in Mathematical Sciences},
  shortjournal = {Commun. Math. Sci.},
  author = {Stuart, Andrew M. and Voss, Jochen and Wilberg, Petter},
  urldate = {2014-12-17},
  date = {2004-12},
  pages = {685--697},
  file = {Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/25XDJS9J/1109885503.html:text/html},
  zmnumber = {1082.65004}
}

@article{Baek2008,
  title = {What makes teachers use technology in the classroom? Exploring the factors affecting facilitation of technology with a Korean sample},
  volume = {50},
  issn = {0360-1315},
  url = {http://www.sciencedirect.com/science/article/pii/S036013150600100X},
  doi = {10.1016/j.compedu.2006.05.002},
  shorttitle = {What makes teachers use technology in the classroom?},
  abstract = {The purpose of this study was to identify factors influencing teachers’ decisions about using technology in the classroom setting and examine the degree to which teaching experience affects these decisions. Specifically, the items employed in this study were derived from the teachers’ perceptions of technology use. We discovered six factors which influenced teachers use technology in their classroom: adapting to external requests and others’ expectations, deriving attention, using the basic functions of technology, relieving physical fatigue, class preparation and management, and using the enhanced functions of technology. Interestingly, these factors do not correspond to the common sense theory of instructional technology. Additionally, we analyzed the patterns of factors’ scores by teachers’ level of teaching experience. From this study we deduced that although the majority of teachers intend to use technology to support teaching and leaning, experienced teachers generally decide to use technology involuntarily in response to external forces while teachers with little experience are more likely to use it on their own will.},
  timestamp = {2015-07-10 18:51:30},
  number = {1},
  journaltitle = {Computers \& Education},
  shortjournal = {Computers \& Education},
  author = {Baek, Youngkyun and Jung, Jaeyeob and Kim, Bokyeong},
  urldate = {2015-07-10},
  date = {2008-01},
  pages = {224--234},
  keywords = {Factors affecting technology use,Technology in education,Technology use in the classroom},
  file = {ScienceDirect Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/BQSWTFG7/login.html:text/html},
  year = 2008
}

@incollection{Held2010,
  title = {Posterior and Cross-validatory Predictive Checks: A Comparison of {MCMC} and {INLA}},
  rights = {©2010 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-7908-2412-4 978-3-7908-2413-1},
  url = {http://link.springer.com/chapter/10.1007/978-3-7908-2413-1_6},
  shorttitle = {Posterior and Cross-validatory Predictive Checks},
  abstract = {Model criticism and comparison of Bayesian hierarchical models is often based on posterior or leave-one-out cross-validatory predictive checks. Cross-validatory checks are usually preferred because posterior predictive checks are difficult to assess and tend to be too conservative. However, techniques for statistical inference in such models often try to avoid full (manual) leave-one-out cross-validation, since it is very time-consuming. In this paper we will compare two approaches for estimating Bayesian hierarchical models: Markov chain Monte Carlo (MCMC) and integrated nested Laplace approximations (INLA). We review how both approaches allow for the computation of leave-one-out cross-validatory checks without re-running the model for each observation in turn. We then empirically compare the two approaches in an extensive case study analysing the spatial distribution of bovine viral diarrhoe (BVD) among cows in Switzerland.},
  timestamp = {2014-12-19 21:36:46},
  booktitle = {Statistical Modelling and Regression Structures},
  publisher = {Physica-Verlag {HD}},
  author = {Held, Leonhard and Schrödle, Birgit and Rue, Håvard},
  editor = {Kneib, Thomas and Tutz, Gerhard},
  urldate = {2014-12-16},
  date = {2010-01-01},
  pages = {91--110},
  langid = {english},
  keywords = {Bayesian hierarchical models,INLA,Leave-one-out cross-validation,MCMC,Posterior predictive model checks,Statistical Theory and Methods,Statistics, general},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/ZWRUSV7A/Held et al. - 2010 - Posterior and Cross-validatory Predictive Checks .pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/PRURGHDF/978-3-7908-2413-1_6.html:text/html}
}

@incollection{Danvy1999,
  title = {Type-Directed Partial Evaluation},
  rights = {©1999 Springer-Verlag Berlin Heidelberg},
  abstract = {Type-directed partial evaluation uses a normalization function to achieve partial evaluation. These lecture notes review its background, foundations, practice, and applications. Of specific interest is the modular technique of offline and online type-directed partial evaluation in Standard ML of New Jersey.},
  timestamp = {2015-03-04 22:31:29},
  number = {1706},
  booktitle = {Partial Evaluation},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer Berlin Heidelberg},
  author = {Danvy, Olivier},
  editor = {Hatcliff, John and Mogensen, Torben Æ and Thiemann, Peter},
  date = {1999},
  pages = {367--411},
  langid = {english},
  keywords = {Programming Languages, Compilers, Interpreters,Programming Techniques},
  file = {chp%3A10.1007%2F3-540-47018-2_16.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/RXNEDATW/chp%3A10.1007%2F3-540-47018-2_16.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/B8DPS5WB/login.html:text/html}
}

@article{mok2014teaching,
  title = {Teaching tip: The flipped classroom},
  volume = {25},
  timestamp = {2015-08-05 02:48:38},
  number = {1},
  journaltitle = {Journal of Information Systems Education},
  shortjournal = {J. Inf. Syst. Educ.},
  author = {Mok, Heng Ngee},
  date = {2014},
  pages = {7},
  year = 2014
}

@article{Roberts1997a,
  title = {Weak convergence and optimal scaling of random walk Metropolis algorithms},
  volume = {7},
  issn = {1050-5164, 2168-8737},
  url = {http://projecteuclid.org/euclid.aoap/1034625254},
  doi = {10.1214/aoap/1034625254},
  abstract = {This paper considers the problem of scaling the proposal distribution of a multidimensional random walk Metropolis algorithm in order to maximize the efficiency of the algorithm. The main result is a weak convergence result as the dimension of a sequence of target densities, n, converges to ∞\ensuremath{\backslash}infty. When the proposal variance is appropriately scaled according to n, the sequence of stochastic processes formed by the first component of each Markov chain converges to the appropriate limiting Langevin diffusion process. The limiting diffusion approximation admits a straightforward efficiency maximization problem, and the resulting asymptotically optimal policy is related to the asymptotic acceptance rate of proposed moves for the algorithm. The asymptotically optimal acceptance rate is 0.234 under quite general conditions. The main result is proved in the case where the target density has a symmetric product form. Extensions of the result are discussed.},
  timestamp = {2014-12-19 21:37:04},
  number = {1},
  journaltitle = {The Annals of Applied Probability},
  shortjournal = {Ann. Appl. Probab.},
  author = {Roberts, G. O. and Gelman, A. and Gilks, W. R.},
  urldate = {2014-11-19},
  date = {1997-02},
  pages = {110--120},
  keywords = {Markov Chain Monte Carlo,Metropolis algorithm,optimal scaling,weak convergence},
  file = {Roberts Gelman Gilks 1997.pdf:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/FZWRVZ82/Roberts Gelman Gilks 1997.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/SWX3NQDG/1034625254.html:text/html},
  mrnumber = {MR1428751},
  zmnumber = {0876.60015}
}

@article{Beckman1976,
  title = {A partial evaluator, and its use as a programming tool},
  volume = {7},
  issn = {0004-3702},
  doi = {10.1016/0004-3702(76)90011-4},
  abstract = {Programs which perform partial evaluation, beta-expansion, and certain optimizations on programs, are studied with respect to implementation and application. Two implementations are described, one “interpretive” partial evaluator, which operates directly on the program to be partially evaluated, and a “compiling” system, where the program to be partially evaluated is used to generate a specialized program, which in its turn is executed to do the partial evaluation. Three applications with different requirements on these programs are described. Proofs are given for the equivalence of the use of the interpretive system and the compiling system in two of the three cases. The general use of the partial evaluator as a tool for the programmer in conjunction with certain programming techniques is discussed.},
  timestamp = {2015-03-04 22:23:48},
  number = {4},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  author = {Beckman, Lennart and Haraldson, Anders and Oskarsson, Östen and Sandewall, Erik},
  date = {1976},
  pages = {319--357},
  file = {ScienceDirect Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/E7T8GVM8/login.html:text/html}
}

@article{Waller1997,
  title = {Log-Linear Modeling with the Negative Multinomial Distribution},
  volume = {53},
  rights = {Copyright © 1997 International Biometric Society},
  issn = {0006-341X},
  doi = {10.2307/2533557},
  abstract = {We develop a negative multinomial sampling plan in which observed cell counts are positively correlated. We show that maximum likelihood estimates of cell means are the same as those found under independent Poisson sampling. There is no maximum likelihood estimate for the shape parameter in general. We propose an estimate of the shape parameter based on the mean and quantiles of Pearson's chi-squared statistic. These techniques are applied to models of cancer incidence for three cities in Ohio and longitudinal health care utilization by a group of senior citizens},
  timestamp = {2015-01-16 23:15:43},
  eprinttype = {jstor},
  eprint = {2533557},
  number = {3},
  journaltitle = {Biometrics},
  shortjournal = {Biometrics},
  author = {Waller, Lance A. and Zelterman, Daniel},
  date = {1997-09-01},
  pages = {971--982},
  file = {JSTOR Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/PP4XZZNA/Waller and Zelterman - 1997 - Log-Linear Modeling with the Negative Multinomial .pdf:application/pdf}
}

@article{Lunn2000,
  title = {{WinBUGS} - A Bayesian modelling framework: Concepts, structure, and extensibility},
  volume = {10},
  shorttitle = {{WinBUGS} - A Bayesian modelling framework},
  abstract = {WinBUGS is a fully extensible modular framework for constructing and analysing Bayesian full probability models. Models may be specified either textually via the BUGS language or pictorially using a graphical interface called DoodleBUGS. WinBUGS processes the model specification and constructs an object-oriented representation of the model. The software offers a user-interface, based on dialogue boxes and menu commands, through which the model may then be analysed using Markov chain Monte Carlo techniques. In this paper we discuss how and why various modern computing concepts, such as object-orientation and run-time linking, feature in the software's design. We also discuss how the framework may be extended. It is possible to write specific applications that form an apparently seamless interface with WinBUGS for users with specialized requirements. It is also possible to interface with WinBUGS at a lower level by incorporating new object types that may be used by WinBUGS without knowledge of the modules in which they are implemented. Neither of these types of extension require access to, or even recompilation of, the WinBUGS source-code.},
  timestamp = {2015-03-25 23:35:56},
  number = {4},
  journaltitle = {Statistics and Computing},
  shortjournal = {Statistics and Computing},
  author = {Lunn, David J. and Thomas, Andrew and Best, Nicky and Spiegelhalter, David},
  date = {2000-10-01},
  pages = {325--337},
  langid = {english},
  keywords = {BUGS,Data Structures, Cryptology and Information Theory,directed acyclic graphs,Markov Chain Monte Carlo,Mathematical Modeling and Industrial Mathematics,Numeric Computing,object-orientation,run-time linking,Statistics, general,type extension,WinBUGS},
  file = {Full Text PDF:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/GQ9WW2SS/Lunn et al. - 2000 - WinBUGS - A Bayesian modelling framework Concepts.pdf:application/pdf;Snapshot:/Users/dturek/Library/Application Support/Zotero/Profiles/mb0txt09.default/zotero/storage/XXJ2SFHE/A1008929526011.html:text/html}
}

@inproceedings{Plummer2003,
  title = {{JAGS}: A program for analysis of Bayesian graphical models using Gibbs sampling},
  volume = {124},
  timestamp = {2015-03-15 21:31:40},
  booktitle = {Proceedings of the 3rd international workshop on distributed statistical computing},
  publisher = {Vienna},
  author = {Plummer, Martyn},
  date = {2003},
  pages = {125}
}

@book{Krajcik2006,
  title = {Project-based learning},
  timestamp = {2015-07-07 19:10:13},
  author = {Krajcik, Joseph S and Blumenfeld, Phyllis C},
  date = {2006},
  year = 2006
}

@article{feingold_tradition_1991,
        title = {Tradition versus {Novelty}: {Universities} and {Scientific} {Societies} in the {Early} {Modern} {Period}},
        shorttitle = {Tradition versus {Novelty}},
        journal = {Revolution and Continuity: Essays in the History and Philosophy of Early Modern Science},
        author = {Feingold, Mordechai},
        year = {1991},
        pages = {45--62}
}
